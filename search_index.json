[["index.html", "Module 1 Nau mai, haere mai. Welcome to BIOSCI 220!", " Module 1 Nau mai, haere mai. Welcome to BIOSCI 220! Module 01: Data exploration, data sovereignty and statistical inference Lecturers: Charlotte Jones-Todd &amp; Jenn Jury Guest lecturer: Nicole Edwards "],["key-information.html", "Key Information Course outline R and RStudio Your teaching team Lectures Assessment", " Key Information Please read this page carefully as it contains important information about the course structure, delivery, and assessment. Please note that we do not send an order to UBIQ to print this course guide for a few reasons: we can make updates to the course guide during semester when hosted online like it is, printing it loses some of the functionality of the course guide with the way the code is presented once printed, and we are also keen to save some trees from unnecessary printing! However, if you want to print it, you’re more than welcome to do so. The easiest way to do this is printing to PDF from your browser. Course outline Living systems are the most complex systems in science, and biology is naturally variable and noisy due to its many internal and external influences. For these reasons, it is difficult to make inferences from and predictions about biological systems. Understanding biology requires computational skills to effectively analyse and interpret data, and multidisciplinary research approaches are becoming more common as a critical key to solving many of the complex problems of studying life and living organisms in today’s world. So, contrary to the popular undergrad biology student beliefs, statistics, mathematics and computational skills are essential in a biologist’s toolkit. To understand modern biological research and findings, and to participate in this research (and get jobs!), skills in working with and visualizing data, learning from data using models, and generating data using simulations of models are crucial. These might be classic statistical models, mathematical models, or inference with process-based models. Biologists also need to be careful and critical thinkers about data and how it is acquired, as well as think critically about the models that we use to try to simplify, and thereby understand, the incredible complexity of biology. BIOSCI 220: Quantitative Biology will introduce you to the programming language R to develop the aforementioned skills, with no coding experience assumed or expected. The aim is to give beginners the confidence to continue learning R and not be afraid of statistics and mathematics! R and RStudio Throughout this course, you will be learning how to program using R and RStudio weekly. You are expected to install and familiarize yourself with these environments before or during the first week of the course. We provide help sessions during week 1 and easy-to-follow instructions for this, please see CANVAS and/or Installing R and RStudio to get started. Note that lab computers will already have R andRStudio installed; however, if you plan to use your personal computer then you will need to install both R and RStudio yourself. Follow the directions in Installing R and RStudio to do so (or come along to a dedicated help session). Another option available is to use RStudio Cloud where, everything is run in a web browser (on a remote server) and doesn’t require you to download the software onto your personal computer. Your teaching team Semester 2, 2024 Module Number Module Content Weeks taught Lecturer 1 R programming and data exploration 1 - 3 Dr Charlotte Jones-Todd 1 Statistical inference 4 - 6 Jenn Jury (course coordinator) 2 Mathematical modelling in biology 7 - 9 Dr Nobuto Takeuchi 3 Model-based inference and critical thinking 10 - 12 Dr Nick Matzke Lectures Lectures are every week: Mondays and Tuesdays 9-10 am in 201-393 You should attend the lectures in-person as there will be in-class activities to assist you with your learning. However, lecture recordings will be available in CANVAS via the Panopto video tab after the lecture has been delivered (typically within 24 hours). Assessment Assessment structure Week Laboratories Quizzes Inspera 1 R help sessions 2 lab quiz 3 lab quiz 4 quiz 5 lab quiz 6 lab quiz Mid-semester break 7 quiz Test 8 lab quiz 9 lab quiz 10 quiz 11 lab quiz 12 lab quiz Exam Period Exam Assessment summary Week of semester Weighting per assessment (%) Total weighting (%) Laboratory 2, 3, 5, 6, 8, 9, 11, 12 5 40 Weekly quiz 2 - 12 inclusive 1 10 Test 7 20 20 Exam Exam period 30 30 The test will be held on Monday 9th September 6.30-8.15pm. The test will cover all of Module 1 taught material only. The exam will cover both Module 2 &amp; 3 taught material only. The delivery mode for the test and the exam is an on-campus, invigilated Inspera assessment. Laboratories There are eight labs in total held in weeks 2, 3, 5, 6, 8, 9, 11 and 12 (i.e., in each three week lecture block there are labs in the last two weeks). Each lab is worth 5% of your final grade (40% in total). Labs will be held in 106-014 (Biology Building, Room 014) during your scheduled SSO lab time, which will be one of Monday 2-5pm, Tuesday 10am-1pm, Tuesday 2-5pm, or Friday 10am-1pm. Labs are three (3) hours and the material is designed to be completed in this time. You are scheduled into a lab stream in SSO. While lab attendance is not compulsory, it is highly recommended you attend your scheduled lab stream. There is teaching staff to support you through the tasks and answer your course-related questions. Weekly quizzes There are eleven weekly quizzes in total. Each quiz is worth 1% and (if you have completed all quizzes) your one (1) quiz that has the lowest mark will be dropped from your final grade. Therefore your weekly quizzes will total 10% towards final grade. The weekly quizzes are due at the end of each week from week 2 - week 12 (due Friday at 10pm). These are designed for you to solidify concepts and practice skills taught each week and thereby keep up with the material. There are practice quizzes for you to attempt prior to sitting the weekly quiz, these are not assessed. There is also a practice quiz for week 1. "],["r-and-rstudio-1.html", "Chapter 1 R and RStudio 1.1 What are R and RStudio? 1.2 Installing R and RStudio 1.3 Getting started with R 1.4 Dealing with data 1.5 Error handling and debugging", " Chapter 1 R and RStudio The purpose of this section is to get you started learning a new language and learning how to program! Throughout BIOSCI220 you will be introduced to tools required to critically analyse and interpret biological data. Learning objectives Define the difference between R and RStudio Express the benefits and issues associated with these software being used in the scientific community. Specifically, summarise the benefits and drawbacks associated with the open-source paradigm, discuss the concept of reproducible research and outline its importance Distinguish between different data types (e.g., integers, characters, logical, numerical) Explain what an R function is; describe what an argument to an R function is Explain what an R package is; distinguish between the functions install.packages() and library() Explain what a working directory is in the context of R Interpret and fix basic R errors such as ## Error in library(fiddler): there is no package called &#39;fiddler&#39; and ## Error in file(file, &quot;rt&quot;): cannot open the connection Use the appropriate R function to read in a .csv data; carry out basic exploratory data analysis using tidyverse (e.g., use the pipe operator, %&gt;% when summarizing a data.frame) Create simple plots of different types of data (e.g., continuous, discrete) 1.1 What are R and RStudio? R is a language, specifically, a programming language. Think of it as the way you can speak to your computer to ask it to carry out certain computations. RStudio is an integrated development environment (IDE). This means it is basically an interface, albeit a fancy one, that makes it easier to communicate with your computer in the language R. The main benefit is the additional features it has that enable you to more efficiently speak R. You can think of R as the engine to RStudio’s dashboard. Note R and RStudio are two different pieces of software; for this course you are expected to download both. As you’d expect, the dashboard depends on the engine (i.e., RStudio depends on R) so you have to download R to use RStudio! 1.1.1 Why are you learning to program? The selling pitch of this course states that …biological research has actually been heavily quantitative for 100+ years… and promises that …it is now essential for biology students to acquire skills in working with and visualizing data, learning from data using models…. We’re not making it up! If you need convincing that quantitative and programming skills are essential to graduate in all scientific disciplines have a read of the following. The Popularity of Data Science Software Data Carpentry for Biologists Throughout this course we will be programming using R. R is a free open-source statistical programming language created by Ross Ihaka (of Ngati Kahungunu, Rangitane and Ngati Pakeha descent) and Robert Gentleman, here at UoA in the Department of Statistics! It is widely used across many scientific fields for data analysis, visualization, and statistical modeling. Proficiency in R will enable you to wrangle and explore datasets, conduct statistical analyses, and create visualizations to communicate your findings. These are all essential tools required in any scientific discipline. TASK Go to the Statistics Department display in the science building foyer (building 302) to see a CD-Rom with the first ever version of R. RStudio is an integrated development environment (IDE) for the R programming language. It serves as a user-friendly workspace, offering additional tools for coding, data visualization, and project management. RStudio simplifies the process of learning R by providing a structured interface with many built-in tools to help organize workflow, fostering a systematic approach to data analysis and research. TASK Research the meaning of open-source software and briefly outline the pros and cons of this in the context of statistical analysis. 1.2 Installing R and RStudio NOTE: RStudio depends on R so there is an order you should follow when you download these software! TASK Follow the instructions below to install R and RStudio. Here are some step-by-step instructional videos you might find useful Software Carpentry install R &amp; RStudio Mac Software Carpentry install R &amp; RStudio Windows Download and install R by following these instructions. Make sure you choose the correct operating system; if you are unsure then please ask either a TA or myself. Download and install RStudio by going here choosing RStudio Desktop Open Source License Free and following instructions. Again if you are unsure then please ask either a TA or myself. Check all is working Open up RStudio from your computer menu, the icon will look something like this (DO NOT use this icon , this is a link to R and will only open a very basic interface) Wait a little and you should see RStudio open up to something similar to the screenshot below Pay close attention to the notes in the screenshot and familiarise yourself with the terms. TASK Once you have both R and Rstudio installed, in the Console next to the prompt type 1:10 and press enter on your keyboard. Your computer should say something back you (in the Console)! What do you think you were asking it to do? Does the output make sense?1 1.3 Getting started with R Open up RStudio from your computer menu, the icon will look something like this . Identify the following panes Console Pane: The console is where you can directly interact with R. You can type and execute R commands line by line, and the results will be displayed in the console pane. It’s an interactive space for immediate feedback and testing. Environment/History Pane: This pane provides information about the current R environment, including a list of variables, data frames, and other objects in your workspace. It also shows the history of commands that you have executed in the console. Files/Plots/Packages/Help Pane: This pane is a multi-purpose pane that can display different tabs depending on your current activity. It can show the file structure of your project, display plots and visualizations, provide information about installed packages, and offer help documentation. These three panes will likely be the ones to appear by default. There is a fourth pane, one which you will have to open yourself. This is the Code/Source Pane and is where you can write, edit, and save your R script or code files. It’s the primary workspace for creating and modifying your R code. TASK Read the text at the top of your Console. What does the very first line say? Mine says version 4.3.1 (2023-06-16) -- \"Beagle Scouts\". Read this blog to find our what this means. 1.3.1 The Code/Source Pane R Scripts (a .r file) Go File &gt; New File &gt; R Script to open up a new Script If you had only three panes showing before, a new (fourth) pane should open up in the top left of RStudio. This file will have a .r extension and is where you can write, edit, and save the R commands you write. It’s a dedicated text editor for your R code (very useful if you want to save your code to run at a later date). The main difference between typing your code into a Script vs Console is that you edit it and save it for later! Remember though the Console is the pane where you communicate with your computer so all code you write will have to be Run here. There are two ways of running a line of code you’ve written in your Script Ensure your cursor is on the line of code you want to run, hold down Ctrl and press Enter. Ensure your cursor is on the line of code you want to run, then use your mouse to click the Run button (it has a green arrow next to it) on the top right of the Script pane. TASK Type 1:10 in your Script and practise running this line of code using both methods above. Not that if you’ve Run the code successfully then your computer will speak back to you each time via the Console. 1.3.2 R basics Term Description Script A file containing a series of R commands and code that can be executed sequentially. Source To execute the entire content of an R script, often done using the “Source” button in RStudio. Running Code The process of executing R commands or scripts to perform specific tasks and obtain results. Console The interactive interface in RStudio where R commands can be entered and executed line by line. Commenting Adding comments to the code using the # symbol to provide explanations or annotations. Comments are ignored during code execution. Assignment Operator The symbol &lt;- or = used to assign values to variables in R. Variable A named storage location for data in R, which can hold different types of values. Data Type The classification of data into different types, such as numeric, character, logical, etc. Object A data structure that holds a specific type of data. Objects are used to store and manipulate data, and they can take various forms depending on the type of information being represented. Logical Operator Symbols like ==, !=, &lt;, &gt;, &lt;=, and &gt;= used for logical comparisons in conditional statements. Function A block of reusable code that performs a specific task when called (e.g., mean(c(3, 4)). Argument The input values that are passed to the function when it is called. Error Handling The process of anticipating, detecting, and handling errors that may occur during code execution. Debugging The process of identifying and fixing errors or bugs in the code. Workspace The current working environment in R, which includes loaded data, variables, and functions. Commenting Comments are notes to yourself (future or present) or to someone else and are, typically, written interspersed in your code. Now, the comments you write will typically be in a language your computer doesn’t understand (e.g., English). So that you can write yourself notes in your Script you need to tell your computer using the R language to ignore them. To do this precede any note you write with #, see below. The # is R for ignore anything after this character. ## IGNORE ME ## I&#39;m a comment ## I repeat I&#39;m a comment ## I am not a cat ## OK let&#39;s run some code 2 + 2 ## [1] 4 ## Hmm maybe I should check this ## @kareem_carr ;-) Now remember when you want to leave your R session you’ll need to Save your Script to use it again. To do this go File &gt; Save As and name your file what you wish (remember too to choose a relevant folder on your computer, or as recommended use the .Rproj set-up as above). Data types Artwork by @allison_horst Here we’re covering data types in R (e.g., integers, doubles/numeric, logical, and characters). Integers are whole values like 1, 0, 220. These are classified \"integer\" or int in R. Numeric values are a larger set of values containing integers but also fractions and decimal values, for example -56.94 and 1.3. These are classified \"numeric\" or num or dbl in R. Logical values are either TRUE or FALSE. These are classified \"logical\" or lgl in R. Characters are text such as “Charlotte”, “BIOSCI220”, and “Statistics is the greatest subject ever”. Note that characters are denoted with the quotation marks around them and are classified \"character\" or chr in R. ## As an example we&#39;re going to ask our computer, using R, what it classified the character string &quot;Charlotte&quot; as class(&quot;Charlotte&quot;) ## [1] &quot;character&quot; Creating Objects Objects are created (or assigned) values using the symbols &lt;- (an arrow formed out of &lt; and -). Like we, typically, write an equation the left-hand side is the Object we’re defining (creating) and the right-hand side is the stuff we’re defining it as. For example, below I’m creating the Object my_name and assigning it the character string of my first name. my_name &lt;- &quot;Charlotte&quot; So now the Object my_name ‘contains’ the value \"Charlotte\". Another assignment to the same object will overwrite the content. my_name &lt;- &quot;Moragh&quot; To check the content of an Object you can simply as your computer to print it out for you (in R). my_name ## [1] &quot;Moragh&quot; Note: R is case sensitive: it treats my_name and My_Name as different objects. An object can be assigned a collection of things: my_names &lt;- c(&quot;Charlotte&quot;, &quot;Moragh&quot;, &quot;Jones-Todd&quot;) my_names ## [1] &quot;Charlotte&quot; &quot;Moragh&quot; &quot;Jones-Todd&quot; some_numbers &lt;- c(1,4,5,13,45,90) some_numbers ## [1] 1 4 5 13 45 90 An Object can also be an entire dataset (see later in this section)! R functions Functions (or commands) perform tasks in R. They take in inputs called arguments and return outputs. Functions often have both mandatory and optional arguments: mandatory arguments must be provided for the function to execute correctly, while optional arguments have default values but can be customized by the user. You can either manually specify a function’s arguments or use the function’s default values. For example, the function seq() in R generates a sequence of numbers. If you just run seq() it will return the value 1. That doesn’t seem very useful! This is because the default arguments are set as seq(from = 1, to = 1). Thus, if you don’t pass in different values for from and to to change this behaviour, your computer just assumes all you want is the number 1. You can change the argument values by updating the values after the = sign. If we try out seq(from = 2, to = 5) we get the result rseq(from = 2, to = 5)` that we might expect. R packages An R package is simply a collection of functions! Typically all focused on a particular type of procedure. The base installation of R comes with many useful packages as standard and these packages will contain many of the functions you will use on a daily basis (e.g., mean(), length()). However, often we wish to do more than base R offer! To do this we need to access all the other amazing packages there are in the Rverse. The Comprehensive R Archive Network(CRAN) is like a centralised library with thousands of books (packages) in stock. To access the contents of a book (package) you first need to request it for (install it into) your local library (your computer): ## note that &lt;the.package.name&gt; is just a placeholder ## and should be replaced by your desired package name install.packages(&#39;&lt;the.package.name&gt;&#39;) Note that you ou can only access books in your local library. To access the knowledge in a particular book (use the function is the package) you need to tell your computer via R to go get the book of the shelf. Then you have access to all the functions it contains! ## note that &lt;the.package.name&gt; is just a placeholder ## and should be replaced by your desired package name library(&lt;the.package.name&gt;) Working directories You need to tell your computer where to look! Look at the top of your Console. You will see something like ~/Desktop/ or C://Users/… (it won’t be an exact match of course). This is the ‘address’ of where your computer is looking. Now, run getwd() and see what output you get (it will be the same as written on the top of your Console pane. This is because getwd() stands for get the current working directory (i.e., the current directory you are currently working in) e.g., getwd() ## [1] &quot;/home/runner/work/BIOSCI220/BIOSCI220&quot; You should ensure that you are aware of which directory you’re working in (which folder RStudio is looking in by default) as this is important later on when we come to reading in files and saving our work! What if you’re not where you want to be (i.e., you are looking in the wrong folder)? Click Session &gt; Set Working Directory &gt; Choose Directory &gt; Chose where you want to go Now notice that something has been written in your Console something similar to setwd(\"~/Git/BIOSCI220/data\"). Now setwd() stands for set your workingdirectory. If you know the address of the directory you want to work in without having to point-and-click you could use this command directly, in this case you’ve used the point-and-click to do it and RStudio has helpfully written out your choices as an R command. Getting help How do we know what a function does? Let’s say we want to learn more about the function mean() (we can take a wild guess at what it calculates, but… what if we didn’t know for sure. There are two ways we can ask within RStudio ?mean() or help(mean) The code above will open up the official R documentation (or man pages) for the function mean(). Every function has a corresponding man page that provides comprehensive information on its usage, required arguments, return value, and often gives user examples. You should always consult the documentation to ensure that what you believe you are asking is indeed what you are asking your computer to do. 1.4 Dealing with data Reading in data from a .csv file First off download the paua.csv file from this link onto your computer (remember which folder you saved it in!) This dataset contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) To read the data into RStudio In the Environment pane click Import Dataset &gt; From Text (readr) &gt; Browse &gt; Choose your file, remembering which folder you downloaded it to &gt; Another pane should pop up, check the data looks as you might expect &gt; Import You should now notice that in the Environment pane there is something listed under Data (this is the name of the data.frame Object containing the data we will explore) Now notice how in the Console a few lines of code have been added. These are the commands you were telling your computer via the point-and-click procedure you went through! Notice the character string inside read_csv()… This is the full ‘address’ of your data (the folder you saved it in). When you tell your computer to look for something you need to tell it exactly where it is! Remember the getwd() command above, this tells you the default location RStudio will look for a file, if your file is not in this folder you have to tell it the full address. Alternatively we could read this data directly into R using the URL (and the readr package, which is part of the tidyverse collection): paua &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv&quot;) 1.4.1 Using functions to look at the data Automatically RStudio has run the command View() for you. This makes your dataset show itself in the top left pane. It’s like looking at the data in Excel. Follow along with the commands below, I recommend that you open up a new Script and use that to write and save your commands for later. Don’t forget to ensure you have read the paua into your session (all commands below assume that your data Object is called paua, if you’ve called it something different then just replace paua with whatever you’ve called it below. Now let’s go ahead and use some functions to ask and answer questions about our data. The first thing you should always do is view any data frames you import. Let’s have a look at your data in the Console paua ## # A tibble: 60 × 3 ## Species Length Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haliotis iris 1.8 1.50 ## 2 Haliotis australis 5.4 11.9 ## 3 Haliotis australis 4.8 5.42 ## 4 Haliotis iris 5.75 4.50 ## 5 Haliotis iris 5.65 5.50 ## 6 Haliotis iris 2.8 2.50 ## 7 Haliotis australis 5.9 6.49 ## 8 Haliotis iris 3.75 5.00 ## 9 Haliotis australis 7.2 8.56 ## 10 Haliotis iris 4.25 5.50 ## # ℹ 50 more rows So, what does this show us? A tibble: 60 x 3: A tibble is a specific kind of data frame in R. Our paua dataset has 60 rows (i.e., 60 different observations). Here, each observation corresponds to a P\\(\\overline{a}\\)ua shell. 3 columns corresponding to 3 variables describing each observation. Species, Length, and Age are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 P\\(\\overline{a}\\)uashells. ``... with 50 more rows indicates there are 50 more rows to see, but these have not been printed (likely as it would clog our screen) Let’s look at some other ways of looking at the data. Using the View() command (recall from above) to scroll through the data in a pop-up viewer View(paua) Using the glimpse() command from the dplyr package for an alternative view library(dplyr) glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;… ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.… ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6… glimpse() will give you the first few entries of each variable in a row after the variable name. Note also, that the data type of the variable is given immediately after each variables name inside &lt; &gt;. TASK! Try running the following operations on the paua data object. What do you think each is asking? head(paua) tail(paua) paua[1:10,] paua[,2] paua$Age 1.5 Error handling and debugging Sometimes rather than doing what you expect it to your computer will return an Error message to you via the Console prefaced with Error in… followed by text that will try to explain what went wrong. This, generally, means something has gone wrong, so what do you do? Read it! THE MESSAGES ARE WRITTEN IN PLAIN ENGLISH (MOSTLY) DO NOT continue running bits of code hoping the issue will go away. IT WILL NOT. Try and work out what it means and fix it (some tips below). Interpreting R errors is an invaluable skill! The error messages are designed to be clear and informative. They aim to tell might be going wrong in the code. These messages typically contain information about the type of error hinting at how you might fix it. For example, if there is a typo in a function or variable name, R will produce an error like \"Error: object 'variable_name' not found.” This suggests that R couldn’t find the specified object variable_name. Understanding error messages involves paying attention to the details, such as the error type (e.g., syntax error, object not found, etc.), and the specific line number where the error occurred! You should always run your code line-by-line, especially if you are new to programming. This means that the execution of each line of code is done in in isolation, providing immediate feedback and pinpointing the exact location where an error occurs. If the meaning or solution to an error isn’t immediately obvious then make use of the documentation (RTFM) and even technology. Online platforms like Stack Overflow or RStudio Community and tools like ChatGPT can often be your friend! It is very unlikely that you are the first one to have encountered the error and some kind soul with have posted a solution online (which the AI bot will have scraped…). However, it’s crucial to approach online answers carefully. You need to first understand the context of your error and use your knowledge about your issue to figure out if the solution is applicable (this gets easier with experience). It is always best to try debugging yourself before blindly following a stranger’s advice. Debugging is a crucial aspect of R programming and the process itself helps solidify your understanding. There are several widely used methods that help identify and resolve issues in your code, two are outlined below. Print debugging involves actively printing out objects and asking the software to display variable values or check the flow of execution. For example, if you get this error message \"Error: object 'variable_name' not found. I might ask R to print out what objects do exist in my workspace, it could be that a previous typo means that the object variable_nam exists. Rubber duck debugging is a useful debugging strategy where you explain your code or problem out loud (as if explaining it to a rubber duck). Honestly it works! The process helps clarify your thoughts and identify issues even before seeking external help. The table below summaries some common R issues and errors and their solutions. Error/Issue Description Solution Syntax Error Incorrect use of R syntax, such as missing or mismatched parentheses, braces, or quotes. Carefully review the code, check for missing or mismatched symbols, and ensure proper syntax is used. Object Not Found Attempting to use a variable or function that hasn’t been defined or loaded into the workspace. Check for typos in variable or function names, ensure the object is created or loaded, and use correct names. Package Not Installed/Loaded Trying to use a function from a package that is not installed or loaded into the environment. Install the required package using install.packages(\"package_name\"), and load it using library(package_name). Undefined Function/Variable Using a function or variable that hasn’t been defined or is out of scope. Define the function or variable, or check its scope within the code. Data Type Mismatch Operating on data of incompatible types, such as performing arithmetic on character data. Ensure that data types are compatible, and consider using functions like as.numeric() or as.character() for conversions. Misuse of Assignment Operator Using the wrong assignment operator (= instead of &lt;- or vice versa). Be consistent with the assignment operator, and use &lt;- for assignment in most cases. Missing Data Dealing with missing values in the data, leading to errors in calculations or visualizations. Handle missing data appropriately, using functions like na.omit() or complete.cases(). Index Out of Bounds Attempting to access an element in a vector or data frame using an index that is out of range. Check the length of the vector or data frame and ensure the index is within bounds. Failure to Load a File Issues with loading a data file using functions like read.csv() or read.table(). Check the file path, file format, and encoding. Confirm that the file exists in the specified location. Incorrect Function Arguments Providing incorrect arguments or parameters to a function, leading to errors. Refer to the function’s documentation to understand the correct parameters and ensure proper usage. Sometimes your computer will return a warning messages to you prefaced “Warning:”. These can sometimes be ignored as they may not affect us. However, READ THE MESSAGE and decide for yourself. Occasionally, also your computer will write you a friendly message, just keeping you up-to date with what it’s doing, again don’t ignore these they might be telling you something useful! TASK Keep a bug diary! Each time you get an error message, see if you can solve it and keep a diary of your attempts and solution. Other resources: optional but recommended R for Data Science RStudio Education An Introduction to R Learning statistics with R: A tutorial for psychology students and other beginners R for Biologists Quantitative Biology: Basic Introduction to R You should have seen the numbers 1 to 10 printed out as a sequence.↩︎ "],["data-wrangling-and-manipulation.html", "Chapter 2 Data wrangling and manipulation 2.1 Exploratory data analysis and data wrangling in the Tidyverse 2.2 Data Visualization 2.3 Plotting using ggplot2 2.4 Bringing things together", " Chapter 2 Data wrangling and manipulation Learning objectives Import data into R from a .csv file using 1) functions and 2) via the menu Apply the principles of tidy data using the tidyverse in R Carry out and interpret the outputs of basic exploratory data analysis using in-built R functions Carry out basic data wrangling techniques in R (e.g., reshaping data, creating new variables, and aggregating information using functions such as the tidyverse functions mutate(), group_by(), and summarize()). Implement tidyverse pipelines using the %&gt;% (pipe) operator Create appropriate and communicate informative data visualizations using R Map the appropriate data structure to a ggplot2 aesthetic Identify the mapping of a variable given a plot Discuss and critique data visualizations 2.1 Exploratory data analysis and data wrangling in the Tidyverse For this, and most other sections, we will be using the tidyverse, which is a collection of R packages that all share underlying design philosophy, grammar, and data structures. They are specifically designed to make data wrangling, manipulation, visualization, and analysis simpler. To install all the packages that belong to the tidyverse run ## request (download) the tidyverse packages from the centralised library install.packages(&quot;tidyverse&quot;) To tell your computer to access the tidyverse functionality in your session run (Note you’ll have to do this each time you start up an R session): ## Get the tidyverse packages from our local library ## Do this every time you start a new session ## and want to use the tidyverse! library(tidyverse) 2.1.1 The pipe operator A nifty tidyverse tool is called the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h() (i.e., they are not actual R functions, but are shown to demonstrate the structure of a piping sequence): This is where the pipe operator %&gt;% comes in handy. %&gt;% takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows: ## These functions are not true R functions but are ## shown to demonstrate the structure of a piping sequence x %&gt;% f() %&gt;% g() %&gt;% h() You would read this sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h(). Let’s first read in the P\\(\\overline{\\text{a}}\\)ua data we looked at in the previous chapter: library(tidyverse) paua &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv&quot;) TASK Revisit the Dealing with data section in the previous chapter and make sure to practice reading the file into R using the different ways given. Make sure to restart R (i.e., close it down completely and open up a fresh clean workspace) each time. Using a tidyverse pipeline we can calculate the mean Age of each Species in the paua dataset via paua %&gt;% group_by(Species) %&gt;% summarize(mean_age = mean(Age)) ## # A tibble: 2 × 2 ## Species mean_age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Haliotis australis 7.55 ## 2 Haliotis iris 4.40 You would read the sequence above as Take the paua data.frame then Use this and apply the group_by() function to group by Species Use this output and apply the summarize() function to calculate the mean (using (mean()) Age of each group (Species), calling the resulting number mean_age 2.1.2 tidy data “Tidy datasets are all alike, but every messy dataset is messy in its own way.” — Hadley Wickham There are three interrelated rules which make a dataset tidy: Each variable must have its own column Each observation must have its own row Each value must have its own cell Illustration from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst Why ensure that your data is tidy? 1) Consistency: using a consistent format aids learning and reproducibility, and 2) simplicity: it’s a format that is well understood by R! “Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets.” — Hadley Wickham, Tidy Data 2.1.3 Introuducing the Palmer penguins library(palmerpenguins) ## this package contains some nice penguin data penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # ℹ 334 more rows ## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; So, what does this show us? A tibble: 344 x 8: A tibble is a specific kind of data frame in R. The penguin dataset has 344 rows (i.e., 344 different observations). Here, each observation corresponds to a penguin. 8 columns corresponding to 3 variables describing each observation. species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 penguins. ``... with 334 more rows indicates there are 334 more rows to see, but these have not been printed (likely as it would clog our screen) To learn more about the penguins read the paper that talks all about the data collection. 2.1.4 Common dataframe manipulations in the tidyverse, using dplyr and tidyr Even from these first few rows of data we can see that there are some NA values. Let’s count the number of NAs. Remember the %&gt;% operator? Here we’re going to be introduced to a few new things the apply() function, the is.na() function, and how R deals with logical values! library(tidyverse) penguins %&gt;% apply(.,2,is.na) %&gt;% apply(.,2,sum) ## species island bill_length_mm bill_depth_mm ## 0 0 2 2 ## flipper_length_mm body_mass_g sex year ## 2 2 11 0 There’s lot going on in that code! Let’s break it down Take penguins then Use penguins as an input to the apply() function (this is specified as the first argument using the .) Now the apply() function takes 3 arguments: the data object you want it to apply something to (in our case penguins) the margin you want to apply that something to; 1 stands for rows and 2 stands for columns, and the function you want it to apply (in our case is.na()). So the second line of code is asking R to apply the is.na() function over the columns of penguins is.na() asks for each value it’s fed is it an NA value; it returns a TRUE if so and a FALSE otherwise The output from the first apply() is then fed to the second apply() (using the .). The sum() function then add them up! R treats a TRUE as a 1 and a FALSE as a 0. So how many NAs do you think there are! Doesn’t help much. To Now we know there are NA values throughout the data let’s remove then and create a new NA free version called penguins_nafree. There is a really handy tidyverse (dplyr) function for this! penguins_nafree &lt;- penguins %&gt;% drop_na() penguins_nafree ## # A tibble: 333 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ℹ 323 more rows ## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Below are some other useful manipulation functions; have a look at the outputs and run them yourselves and see if you can work out what they’re doing. filter(penguins_nafree, island == &quot;Torgersen&quot; ) ## # A tibble: 47 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ℹ 37 more rows ## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; summarise(penguins_nafree, average_bill_length = mean(bill_length_mm)) ## # A tibble: 1 × 1 ## average_bill_length ## &lt;dbl&gt; ## 1 44.0 group_by(penguins_nafree, species) ## # A tibble: 333 × 8 ## # Groups: species [3] ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ℹ 323 more rows ## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Often we want to summarise variables by different groups (factors). Below we Take the penguins_nafree data then Use this and apply the group_by() function to group by species Use this output and apply the summarize() function to calculate the mean (using (mean()) bill length (bill_length_mm) of each group (species), calling the resulting number average_bill_length penguins_nafree %&gt;% group_by(species) %&gt;% summarise(average_bill_length = mean(bill_length_mm)) ## # A tibble: 3 × 2 ## species average_bill_length ## &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.6 We can also group by multiple factors, for example, penguins_nafree %&gt;% group_by(island,species) %&gt;% summarise(average_bill_length = mean(bill_length_mm)) ## # A tibble: 5 × 3 ## # Groups: island [3] ## island species average_bill_length ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Biscoe Adelie 39.0 ## 2 Biscoe Gentoo 47.6 ## 3 Dream Adelie 38.5 ## 4 Dream Chinstrap 48.8 ## 5 Torgersen Adelie 39.0 What about getting more complicated? I suggest you run the code below one pipe at a time to work out what each function is doing and data it is acting on. penguins_nafree %&gt;% filter(., sex != &quot;male&quot;) %&gt;% select(c(&quot;species&quot;, &quot;island&quot;, &quot;body_mass_g&quot;)) %&gt;% group_by(species, island) %&gt;% summarise(total_mass_g = sum(body_mass_g)) %&gt;% pivot_wider(names_from = c(island), values_from = total_mass_g) ## # A tibble: 3 × 4 ## # Groups: species [3] ## species Biscoe Dream Torgersen ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie 74125 90300 81500 ## 2 Chinstrap NA 119925 NA ## 3 Gentoo 271425 NA NA 2.2 Data Visualization “…have obligations in that we have a great deal of power over how people ultimately make use of data, both in the patterns they see and the conclusions they draw.” — Michael Correll, Ethical Dimensions of Visualization Research “Clutter and confusion are not attributes of data - they are shortcomings of design.” — Edward Tufte “Scientific visualization is classically defined as the process of graphically displaying scientific data. However, this process is far from direct or automatic. There are so many different ways to represent the same data: scatter plots, linear plots, bar plots, and pie charts, to name just a few. Furthermore, the same data, using the same type of plot, may be perceived very differently depending on who is looking at the figure. A more accurate definition for scientific visualization would be a graphical interface between people and data.” — Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures “message and readability of the figure is the most important aspect while beauty is only an option” — Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures 2.2.1 Exploratory plots (for your own purposes) Exploratory plots are just for you, they focus solely on data exploration. They Don’t Have to Look Pretty: These plots are only needed to reveal insights. Just Needs to Get to the Point: Keep the plots concise and straightforward. Avoid unnecessary embellishments or complex formatting. Explore and Discover New Data Facets: Use exploratory plots to uncover hidden patterns, trends, or outliers in the data. Employ different plot types to reveal various facets and aspects of the dataset. Help Formulate New Questions: Use exploratory plots as a tool to prompt new questions and hypotheses. As you discover patterns, think about what additional questions these findings raise for further investigation. Let’s use a base R command (i.e., functions shipped with R) to create a boxplot of our paua data. boxplot(Age ~ Species, data = paua) So what have we asked our computer to do here? We are using the function boxplot() and supplying two arguments: Age ~ Species, and data = paua. The first argument Age ~ Species is how R understands equations (e.g., \\(y = x\\)) and is asking for the variable Age to be plotted by the variable Species. The second argument data = paua specifies from which data object to grab the variables from (in this case the paua object we created above). TASK Run each line of the following code and comment briefly on what each plot shows. boxplot(Length ~ Species, data = paua) boxplot(Age ~ Species, data = paua) plot(Age ~ Length, data = paua) boxplot(Age ~ Length, data = paua) plot(paua$Age) 2.2.2 Explanatory plots Explanatory plots are mainly for others. These are the most common kind of graph used in scientific publications. They should Have a Clear Purpose: Define a clear and specific purpose for your plot. Identify what scientific question or hypothesis the plot is addressing. Avoid unnecessary elements that do not contribute to this purpose. Be Designed for the Audience: Tailor the design of your plot to the characteristics and expertise of your audience. Consider their familiarity with technical terms, preferred visualizations, and overall scientific background. Be Easy to Read: Prioritize readability by using legible fonts, appropriate font sizes, and clear labels. Ensure that the axes are well-labeled, and use a simple and straightforward layout. Avoid clutter and unnecessary complexity that could hinder comprehension. Not Distort the Data: Maintain the integrity of the data by avoiding distortion in your plot. Ensure that scales and proportions accurately represent the underlying data, preventing misleading visualizations. Help Guide the Reader to a Particular Conclusion: Structure your plot in a way that guides the reader toward the intended conclusion. Use visual elements such as annotations, arrows, or emphasis to highlight key findings and lead the reader through the data interpretation process. Answer a Specific Question:Construct your plot with a specific research question in mind. The plot should directly address and answer this question, providing a clear and unambiguous response. Support an Outlined Decision: If the plot is intended to support decision-making, clearly outline the decision or action it is meant to inform. The plot should provide relevant information that aids in making well-informed decisions based on the presented data. Plots by Cedric Scherer and mentioned on this blog The table below summarises Ten Simple Rules for Better Figures, a basic set of rules to improve your visualizations. Rule Name Rule Description Know Your Audience Understand the characteristics and expertise of your audience to tailor the figure accordingly. Identify Your Message Clearly define the main message or takeaway that you want the audience to understand from the figure. Adapt the Figure to the Support Medium Tailor the figure’s complexity and design to suit the medium it will be presented in (e.g., print, online). Captions Are Not Optional Craft informative captions that provide essential details and context for interpreting the figure. Do Not Trust the Defaults Adjust default settings to optimize the visual elements of the figure, such as colors, scales, and labels. Use Color Effectively Apply color purposefully, taking into account accessibility considerations and cultural interpretations. Do Not Mislead the Reader Avoid creating misleading visualizations and be aware of formulas to measure the potential misleading nature of a graph. There are formulas to measure how misleading a graph is! Avoid Chartjunk Eliminate unnecessary decorations and embellishments in the figure that do not contribute to the message. Message Trumps Beauty Prioritize conveying a clear message over making the figure aesthetically pleasing. Get the Right Tool Choose the appropriate visualization tool (e.g., R) or chart type that best communicates the data and intended message. TASK Read this short paper and find examples in your choice of literature of one or more of the rules in action. 2.3 Plotting using ggplot2 ggplot2 is an R package for producing statistical, or data, graphics; it has an underlying grammar based on the Grammar of Graphics Every ggplot2 plot has three key components: data, A set of aesthetic mappings between variables in the data and visual properties, and At least one layer which describes how to render each observation. Layers are usually created with a geom_* function. Every plot created using ggplot2 code requires the three components above: library(ggplot2) ## load package ggplot(data = &lt;your_data&gt;, aes(&lt;specify_aesthetics&gt;)) + ## initialize plot specifying data and aesthetics geom_&lt;specify_geom&gt;() ## add a layer (or geom) Note the + operator used to add a layer onto a ggplot object (see below for examples), it has a specific purpose in a ggplot2 context. It should not be confused with the pipe operator %&gt;% which takes the output of one function and passes it as the first argument to the next function (see The pipe operator). The table below summaries the most common geoms and gives examples of when they are most likely appropriate. geom_* Description Typical Use Case Mapping Aesthetics geom_point Displays individual data points as dots on the plot. Visualizing individual data points in a scatter plot. aes(x = ..., y = ...): X and Y coordinates. geom_line Connects data points with lines, useful for showing trends or relationships between continuous variables. Representing trends or relationships over continuous variables. aes(x = ..., y = ...): X and Y coordinates. geom_bar Creates bar charts, displaying the frequency or count of data within categorical groups. Illustrating the distribution of categorical data. aes(x = ..., y = ...): X-axis (categorical) and Y-axis (count or frequency). geom_histogram Constructs histograms, visualizing the distribution of continuous variables by dividing them into bins. Examining the distribution of a single continuous variable. aes(x = ..., y = ...): X-axis (continuous) and Y-axis (count or frequency). geom_boxplot Generates box plots, illustrating the distribution of a continuous variable and highlighting outliers. Summarizing the distribution of continuous variables. aes(x = ..., y = ...): X-axis (categorical) and Y-axis (continuous). geom_area Fills the area between a line and the x-axis, useful for visualizing accumulated quantities or proportions. Depicting cumulative or proportional relationships. aes(x = ..., y = ...): X and Y coordinates. geom_text Adds text labels to the plot, providing additional information about specific data points. Annotating specific points or adding labels to the plot. aes(x = ..., y = ..., label = ...): X, Y coordinates, and text labels. geom_smooth Fits and overlays a smoothed line (e.g., LOESS or linear regression) on the data points. Highlighting trends and capturing patterns in the data. aes(x = ..., y = ...): X and Y coordinates. geom_violin Produces violin plots, displaying the distribution of a continuous variable across different categories. Visualizing the distribution and density of data within categories. aes(x = ..., y = ...): X-axis (categorical) and Y-axis (continuous). geom_segment Draws line segments between specified points, useful for indicating relationships or connections in the data. Illustrating relationships between two sets of coordinates. aes(x = ..., y = ..., xend = ..., yend = ...): Start and end coordinates for the segments. 2.3.1 Plotting the palmerpenguins data You might find this application useful Recall we used specified an NA-free version of the dataset: penguins_nafree &lt;- penguins %&gt;% drop_na() First off, let’s create a scatter plot. The code below takes the penguins_nafree dataset (created above) and maps the body mass variable body_mass_g to the x-axis and flipper length variable flipper_length_mm to the y-axis. Then, it adds points to the plot to represent each data point in the dataset. Therefore, the plot shows the relationship between body mass (g) and flipper length (mm) for the penguins in our dataset penguins_nafree. ggplot(penguins_nafree,aes(x = body_mass_g, y = flipper_length_mm)) + ## data &amp; aesthetics geom_point() ## geom Breaking the code above down we have ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm)) initializes the plot using the ggplot() function. It specifies the data frame penguins_nafree and the aesthetics mapping: setting the x-axis (x) to the body_mass_g variable and the y-axis (y) to the flipper_length_mm variable. We then add (using the + operator) a layer to the plot using geom_point(), which specifies that the data points should be represented as points on the plot. Each point corresponds to a row in the dataset, with x and y coordinates determined by the aesthetics specified in the previous line. But, what about adding some personal touches? We can add more appropriate axis labels can be specified using xlab() and ylab() and even customizes the color scale using scale_color_manual(): ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point(aes(color = species),size = 2) + scale_color_manual(values = c(&quot;darkorange&quot;,&quot;darkorchid&quot;,&quot;cyan4&quot;), name = &quot;&quot;) + xlab(&quot;Bill length (mm)&quot;) + ylab(&quot;Bill depth (mm)&quot;) Breaking down the lines of code here we have that Again ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm)) initializes the plot using the ggplot() function. It specifies the data frame penguins_nafree and the aesthetics mapping: setting the x-axis (x) to the body_mass_g variable and the y-axis (y) to the flipper_length_mm variable. geom_point(aes(color = species), size = 2) adds a layer to the plot using geom_point(). It specifies that the data points should be represented as points on the plot. The argument aes(color = species) argument the species variable to the color of the points, and size = 2 sets the size of the points. scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\"), name = \"\") customizes the color scale of the points by setting the manually via the argument values = c(\"darkorange\",\"darkorchid\",\"cyan4\"). Here the three specified colors are used for the three different species of penguins passed on from the previous aesthetic. The argument name = \"\" removes the legend title. Finally xlab(\"Bill length (mm)\") and ylab(\"Bill depth (mm)\") manually set the x-axis and y-axis labels. TASKTry specifying some different colours in scale_color_manual() in the code above. Research Hex colour codes and try using these instead of colour names. Now let’s move onto plotting different types of data (e.g., continuous and discrete) on the same plot. We can even add some easy custom themeing using in-built ggplot2 functionality (e.g., theme_classic()). The code below, generates a boxplot that compares the distribution of flipper lengths (mm) for the different penguin species. ## boxplot ggplot(penguins_nafree,aes(x = species, y = flipper_length_mm)) + geom_boxplot() + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;&quot;) + theme_classic() Breaking the code above down we have ggplot(penguins_nafree, aes(x = species, y = flipper_length_mm)) initializes the plot using the ggplot() function. It uses the penguins_nafree dataset and sets the x-axis (x) to the species variable and the y-axis (y) to the flipper_length_mm variable. geom_boxplot() adds a layer to the plot creating boxplots for each level of the species (x-axis) variable (species). ylab(\"Flipper length (mm)\") + xlab(\"\") manually set the y-axis label to \"Flipper length (mm)\" and remove the x-axis label (setting it to an empty string \"\"). theme_classic() applies the so called classic theme to the plot, changing the appearance of the background and gridlines etc. from the default ones above. TASK Type theme_ into your Console and wait for RStudio to attempt to autocomplete for you. Run down the list of in-built themes and try them instead of theme_classic() above. Rather than a boxplot we are going to create a violin plot of the same variables using geom_violin() (we use another theme this time theme_minimal()). Violin plots are more flexible than boxplots as they better show the shape (i.e., distribution) of our data. ## violin plot ggplot(penguins_nafree,aes(x = species, y = flipper_length_mm)) + geom_violin() + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;&quot;) + theme_minimal() ## another theme... TASK What can you say about the distribution of penguin flipper length. Do you this the distributions are symmetrical, skewed, bimodal? The code below plots a histogram of flipper lengths (mm) for penguins, with different colors representing different species. We make the colours transparent using the alpha argument so that we can see the overlap in distributions. There are also another couple of new functions and arguments used, breaking the code down line by line we have ggplot(penguins_nafree, aes(x = flipper_length_mm)) initializes the plot using the ggplot() function. We specify the penguins_nafree dataset and map the x-axis (x) to the flipper_length_mm variable. geom_histogram(aes(fill = species), alpha = 0.5, position = \"identity\") adds a layer to the plot using geom_histogram(), which creates a histogram of flipper lengths. The aes(fill = species) argument colors the bars based on the species variable. Setting alpha = 0.5 adjusts the transparency (which ranges from 0 to 1) of the bars. Finally, the less obvious use of and position = \"identity\" places the bars directly adjacent to each other. xlab(\"Flipper length (mm)\") sets the x-axis label. scale_fill_brewer(palette = \"Dark2\", name = \"Species\") specifies a custom color scale mapped to the fill aesthetic. It sets the color palette to \"Dark2\" and assigns the legend title as \"Species\". theme_light() sets another in-built theme changing the appearance of the background and gridlines etc. ## histogram, with a colorblind friendly palette ggplot(penguins_nafree,aes(x = flipper_length_mm)) + geom_histogram(aes(fill = species), alpha = 0.5, position = &quot;identity&quot;) + xlab(&quot;Flipper length (mm)&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;, name = &quot;Species&quot;) + theme_light() ## and another theme.... TASK We should always avoid using similarly bight red and green colours: they may not be distinguishable for red-green colorblind readers. Using ggplot2 we can access a whole range of colourblind friendly palettes: one package that has a whole range is RColorBrewer install it then try running RColorBrewer::display.brewer.all(colorblindFriendly = TRUE) what do you think you’ve asked your computer to show you? We’ve seen that there are three factor variables in the dataset: species, island, and sex. To count the number of penguins of each species and sex on each island we could use the following pipeline. penguins_nafree %&gt;% count(species, sex, island) ## # A tibble: 10 × 4 ## species sex island n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie female Biscoe 22 ## 2 Adelie female Dream 27 ## 3 Adelie female Torgersen 24 ## 4 Adelie male Biscoe 22 ## 5 Adelie male Dream 28 ## 6 Adelie male Torgersen 23 ## 7 Chinstrap female Dream 34 ## 8 Chinstrap male Dream 34 ## 9 Gentoo female Biscoe 58 ## 10 Gentoo male Biscoe 61 However, it’s not too easy to compare the raw numbers here; so what about a bar graph? ggplot(penguins_nafree, aes(x = species, fill = sex)) + geom_bar(alpha = 0.8, position = &quot;dodge&quot;) + facet_wrap(~island) + ## what are we doing here? xlab(&quot;&quot;) + theme_linedraw() + ## remember themes... scale_fill_manual(values = c(&quot;cyan4&quot;,&quot;darkorange&quot;), name = &quot;Sex&quot;) TASK Before reading the breakdown below write out your own and then compare the two. Breaking down the code above we have ggplot(penguins_nafree, aes(x = species, fill = sex)) initializes the plot using the ggplot() function, specifying the dataset penguins_nafree. It maps the x-axis (x) to the species variable and fills the bars based on the sex variable. geom_bar(alpha = 0.8, position = \"dodge\") adds a layer to the plot using geom_bar(). The alpha = 0.8 argument adjusts the transparency of the bars, and position = \"dodge\" places the bars side by side for each species. facet_wrap(~island) creates a faceted plot where separate plots are generated for each level of the island variable. xlab(\"\") sets the x-axis label as an empty string \"\". theme_linedraw() applies a linedraw theme to the plot. scale_fill_manual(values = c(\"cyan4\", \"darkorange\"), name = \"Sex\")customizes the fill color scale and sets the colors for the two levels of the sex variable assigning the legend title as \"Sex\". TASK Run each code chunk below. Map each line to a component in the plot produced. Can you improve the plots? ggplot(penguins_nafree,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme ggplot(penguins_nafree,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_jitter() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_violin() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) 2.4 Bringing things together 2.4.1 More on pipelines and operators In this chapter you’ve been introduced to the pipe operator %&gt;% and the use of the + operator adding layers to a ggplot. Recall that %&gt;% is used for chaining operations in a data manipulation pipeline, whilst + is used for adding layers to a plot in the ggplot2 framework. They serve different purposes, but we can combine them! For example, we can pipe a data object using %&gt;% into a ggplot workflow and then use + to add layers. library(tidyverse) library(palmerpenguins) penguins %&gt;% na.omit() %&gt;% filter(., island == &quot;Biscoe&quot; ) %&gt;% ggplot(., aes(y = body_mass_g, x = species)) + geom_boxplot() + labs(title = &quot;Penguin Body Mass by Species on Biscoe&quot;, y = &quot;Body Mass (g)&quot;, x = &quot;&quot;) TASK Run the code below. It will give you an error, can you figure out what the cause is? library(tidyverse) library(palmerpenguins) penguins %&gt;% na.omit() %&gt;% filter(., island == &quot;Biscoe&quot; ) %&gt;% ggplot(., aes(y = body_mass_g, x = species)) %&gt;% geom_boxplot() %&gt;% labs(title = &quot;Penguin Body Mass by Species on Biscoe&quot;, y = &quot;Body Mass (g)&quot;, x = &quot;&quot;) 2.4.2 What to look at in a plot (a taster) In many of the following chapters you will hear a lot about variability or variation. Variation is simply a measure of spread (i.e., how far apart the observations are from each other). In particular, we often talk about deviation from the mean (i.e., how far away from the average value the observations are) as we are often interested in how large this deviation is relative to the information we have. In Chapter 5 you will learn about ways of formally testing this. However, with the data visualization skills you’ve just learnt we can already begin to critically evaluate our data by simply looking at plots. Between-group variation is a measure of the variability between different groups; it assesses how much the means of different groups differ from each other. Higher between-group variation suggests that the groups are not similar. However, before we can conclude anything relating to similarity, or not, of groups we also have to consider within-group variation. Within group variation measures the variability among individual data points within each group; it measures how much individual data points within each group differ from the group’s mean. Lower within-group variation suggests that the data points within each group are less scattered around the group’s mean Other resources: optional but recommended Ihaka Lecture Series 2023 Danielle Navarro – Unpredictable paintings: Making generative artwork in R Ihaka Lecture Series 2023 Chris McDowall – What’s Behind the Map: The Process of Data Visualisation ggplot2 cheatsheet Elegant Graphics for Data Analysis Using ggplot2 to communicate your results Teacups, giraffes, and statistics: free online introductory level R and statistics modules "],["ethical-data-practice.html", "Chapter 3 Ethical data practice 3.1 Accuracy and Honesty 3.2 Respectful Handling of Data 3.3 Awareness of Consequences", " Chapter 3 Ethical data practice Learning objectives Define reproducibility in the context of scientific research List, discuss, and employ good coding practice Define data sovereignty and explain this in relation to a researcher’s obligation when collecting, displaying, and analysing data Define and discuss Māori Data Sovereignty principles Discuss the responsibility of researchers in presenting and communicating research Critically evaluate science publications by assessing the appropriateness and accuracy of conclusions drawn When we handle data we should always be accurate, honest, respectful and act responsibly. That seems obvious, right? But what does this mean in practice? There are many ongoing discussions in the scientific community around ethical data practice and what it entails. It is a hugely important subject, and in many ways has a long way to go. We only touch briefly on some aspects of ethical data practice in this section, presenting the suggestions and thoughts of trailblazing communities and organisations in those areas. Accuracy and Honesty: Being accurate and honest in your analyses and conclusions. Respectful Handling of Data: Recognize that data may represents people or beliefs or behaviour and be respectful of this. Awareness of Consequences: Considering the ethical implications and societal impact of your research. TASK Read Ethical Guidelines for Statistical Practice and outline which principle(s) you feel directly relate to your current career pathway. 3.1 Accuracy and Honesty Honesty is an expectation: we expect honesty from you and you expect the same from your teaching team. Honesty is an expectation in any scientific discipline as is accuracy. These are morals, ethical principles we should abide by. But this course isn’t here to discuss philosophy or character development. This course, in particular this section of the course, is to expose you to the tools and principles that will aid you in your own pursuit of ethical data practice. This course doesn’t , teaching you the tools so that your analysis is reproducible goes someway towards accuracy. This because, reproducibility promotes transparency, facilitates error detection and correction, and contributes to the overall reliability and accuracy of your research findings. 3.1.1 Reproducibility “Reproducibility, also known as replicability and repeatability, is a major principle underpinning the scientific method. For the findings of a study to be reproducible means that results obtained by an experiment or an observational study or in a statistical analysis of a data set should be achieved again with a high degree of reliability when the study is replicated. … With a narrower scope, reproducibility has been introduced in computational sciences: Any results should be documented by making all data and code available in such a way that the computations can be executed again with identical results.” — Reprodicibility, Wikipedia Reproducibility is a stepping stone towards ensuring accuracy. This is because, reproducibility promotes transparency, facilitates error detection and correction, and contributes to the overall reliability and accuracy of your research findings. Establishing good practice when dealing with data and code right from the beginning is essential. Good practice 1) ensures that data is collected, processed, and stored accurately and consistently, which helps maintain the quality and integrity of the data throughout its lifecycle; and 2) creates a robust code base, which can be easily understood and adapted as the project progresses, which leads to faster development. 3.1.2 Good coding practice Always start with a clean workspace Why? So your ex (code) can’t come and mess up your life! To ensure this go to Tools &gt; Global Options and uncheck the highlighted options. Why? Because, this is not reproducible, does NOT create a fresh R process, makes your script vulnerable, and it will come back to bite you. TASK Below are two quotes from Jenny Bryan, an R wizard which reference two snippets of R code. Find out what each snippet does and why Jenny is so against them. If the first line of your R script is setwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\") I will come into your office and SET YOUR COMPUTER ON FIRE 🔥. — Jenny Bryan, Tidyverse blog, workflow vs script If the first line of your R script is rm(list = ls()) I will come into your office and SET YOUR COMPUTER ON FIRE 🔥. — Jenny Bryan, Tidyverse blog, workflow vs script A project-oriented workflow in R refers to a structured approach to organizing and managing your code, data, and analyses. This helps improve reproducibility and the overall efficiency of your work. Within this it is essential essential to write code that is easy to understand, maintain, and share. To do so, coding best practice is to follow the 5 Cs by being Clear Code Clarity: Write code that is easy to read and understand. Use meaningful variable and function names that convey the purpose of the code. Avoid overly complex or ambiguous expressions. Comments: Include comments to explain the purpose of your code, especially for complex or non-intuitive sections. Comments should add value without stating the obvious. Concise: Avoid Redundancy: Write code in a way that avoids unnecessary repetition. Reuse functions and use loops or vectorized operations when appropriate to reduce the length of your code. Simplify Expressions: Simplify complex expressions and equations to improve readability. Break down complex tasks into smaller, manageable steps. Consistent: Coding Style: Adhere to a consistent coding style throughout your project. Consistency in indentation, spacing, and naming conventions makes the code visually coherent. Function Naming: Keep naming conventions consistent. If you use camelCase for variable names, continue to use it consistently across your codebase. Correct: Error Handling: Implement proper error handling to ensure that your code gracefully handles unexpected situations. Check for potential issues, and provide informative error messages. Testing: Test your code to ensure it produces the correct output. Use tools like unit tests (e.g., with testthat) to verify that your functions work as intended. Conformant: Follow Best Practices: Adhere to best practices and coding standards in the R community. For example, follow the tidyverse style guide or the Google R Style Guide. Package Guidelines: If you’re creating an R package, conform to package development guidelines. Use the usethis package to help set up your package structure in a conformant way. There are many other good practice tips when it comes to coding these include ensuring your code is modular, implementing unit testing, automating workflows and implementing version control. These topics, however are beyond the scope of this course (take BIOSCI738 if you’d like to learn more about them). 3.2 Respectful Handling of Data 3.2.1 Data sovereignty The term data sovereignty has a hugely broad range of connotations. We do not aim to cover them all in this course. Typically, data sovereignty is focused on the understanding and adhering to the legal and ethical considerations associated with data collection, storage, and analysis in different jurisdictions. Researchers are expected follow and abide by the best ethical and legal practice whilst respecting individuals’ privacy and rights. “For Indigenous peoples, historical encounters with statistics have been fraught, and none more so than when involving official data produced as part of colonial attempts at statecraft.” — Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. Indigenous data sovereignty refers to the right or interest indigenous peoples and nations have to govern the collection, ownership, and application of their own data. As we are in Aotearoa this is most pertinent in the terms of māori data sovereignty. 3.2.2 Māori Data Sovereignty principles “Māori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.” — Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy Te Tiriti o Waitangi/Treaty of Waitangi obliges the Government to actively protect taonga, consult with Māori in respect of taonga, give effect to the principle of partnership and recognize Māori rangatiratanga over taonga. TASK There are many ongoing discussions that surround the Te Tiriti o Waitangi/Treaty of Waitangi, focused mainly on whether the statements of principles it outlines have been upheld. What do you think? Can you find a recent event/news article that relates to your studies, which either clearly upholds, or not, what you deem to be the correct ethical practice here? Māori Data Sovereignty principles inform the recognition of Māori rights and interests in data, and promote the ethical use of data to enhance Māori wellbeing. The Te Mana o te Raraunga Model was developed to align Māori concepts with data rights and interests, and guide agencies in the appropriate use of Māori data. Below are the guiding principles outlined by Te Mana o te Raraunga Model in th e Principles of Māori Data Sovereignty. Rangatiratanga (authority) Māori have an inherent right to exercise control over Māori data and Māori data ecosystems. This right includes, but is not limited to, the creation, collection, access, analysis, interpretation, management, security, dissemination, use and reuse of Māori data. Decisions about the physical and virtual storage of Māori data shall enhance control for current and future generations. Whenever possible, Māori data shall be stored in Aotearoa New Zealand. Māori have the right to data that is relevant and empowers sustainable self-determination and effective self-governance Whakapapa (relationships) All data has a whakapapa (genealogy). Accurate metadata should, at minimum, provide information about the provenance of the data, the purpose(s) for its collection, the context of its collection, and the parties involved. The ability to disaggregate Māori data increases its relevance for Māori communities and iwi. Māori data shall be collected and coded using categories that prioritise Māori needs and aspirations. Current decision-making over data can have long-term consequences, good and bad, for future generations of Māori. A key goal of Māori data governance should be to protect against future harm. Whanaungatanga (obligations) Individuals’ rights (including privacy rights), risks and benefits in relation to data need to be balanced with those of the groups of which they are a part. In some contexts, collective Māori rights will prevail over those of individuals. Individuals and organisations responsible for the creation, collection, analysis, management, access, security or dissemination of Māori data are accountable to the communities, groups and individuals from whom the data derive Kotahitanga (collective benefit) Data ecosystems shall be designed and function in ways that enable Māori to derive individual and collective benefit. Build capacity. Māori Data Sovereignty requires the development of a Māori workforce to enable the creation, collection, management, security, governance and application of data. Connections between Māori and other Indigenous peoples shall be supported to enable the sharing of strategies, resources and ideas in relation to data, and the attainment of common goals. Manaakitanga (reciprocity) The collection, use and interpretation of data shall uphold the dignity of Māori communities, groups and individuals. Data analysis that stigmatises or blames Māori can result in collective and individual harm and should be actively avoided. Free, prior and informed consent shall underpin the collection and use of all data from or about Māori. Less defined types of consent shall be balanced by stronger governance arrangements. Kaitiakitanga (guardianship) Māori data shall be stored and transferred in such a way that it enables and reinforces the capacity of Māori to exercise kaitiakitanga over Māori data. Ethics. Tikanga, kawa (protocols) and mātauranga (knowledge) shall underpin the protection, access and use of Māori data. Māori shall decide which Māori data shall be controlled (tapu) or open (noa) access. 3.3 Awareness of Consequences Considering the implications and societal impact of your research includes ensuring that any conclusions you draw are appropriately and accurately balanced. Consider the previous chapter Data Visualization, the guiding principles of making informed visualizations included not misleading readers and prioritizing conveying a clear message. These speak to being mindful of how your figures may be perceived and presenting your data ethically and responsibly. Your responsibilities go beyond just making figures, they extend to the methods and inferences you draw. Learning how to communicate science is a key and invaluable skill. Siouxsie Wiles, is an award winning science communicator and is perhaps best known for stepping up during the pandemic giving us information about the virus and advice on how to beat it. “I assumed it would be through my research by helping develop a new antibiotic. But through the pandemic, I’ve learned that I can have a huge impact globally by doing good science communication.” — Associate Professor Siouxsie Wiles 2 Below is a case study in science (mis)communication. 3.3.1 Case study Asthma carbon footprint ‘as big as eating meat’ is the headline of this news story published on the BBC website. The article is based research outlined in this published paper which in turn cites this paper for an estimate of the carbon footprint reduced by an individual not eating meat. It is not unreasonable to assume that many people would interpret this to mean that the total global carbon footprint due to eating meat is equal to the total carbon footprint due to the use of asthma inhalers. However, this is not what they mean. They mean that an individual deciding not to eat meat reduces their carbon footprint as much as an asthmatic individual deciding not to use an inhaler. There are far more meat consumers compared to inhaler users and so the overall carbon footprint associated with meat consumption is much greater. However, the claim that not eating meat reduces someone’s carbon footprint about the same amount as not using inhalers is questionable. Yet, both the BBC article and the paper make this claim. “And at the individual level, each metered-dose inhaler replaced by a dry powder inhaler could save the equivalent of between 150kg and 400kg (63 stone) of carbon dioxide a year - similar to the carbon footprint reduction of cutting meat from your diet.” — Asthma carbon footprint as big as eating meat “Changing one MDI device to a DPI could save 150–400 kg CO2 annually; roughly equivalent to installing wall insulation at home, recycling or cutting out meat.” — Wilkinson et al (2019) who cite Wynes and Nicholas (2017) Now, the the carbon footprint of eating meat is estimated as 300–1600 kg CO2 annually by this paper (see Table 1). And so the two claims don’t really match up. Moreover, what is being suggested by the article? That should asthmatics should think about ceasing their medication in the same way many people are trying to reduce meat consumption?!? In this section we’ve discussed how ethical data practice involves accuracy, respect, and clear communication. There is one other component that should be considered here and that is consequence. The two options in this case study are not balanced because they have very different consequences: Not eating meat is (possibly) good for you and is also good for the planet, but Not taking your inhaler is (probably) much worse for your health. TASK Watch this lecture Algorithmic fairness: Examples from predictive models for criminal justice and summarise the key points made. Can you think of a recent story that highlights the issues raised? Other resources: optional but recommended Indigenous Data Sovereignty and Policy Good data practices for Indigenous data sovereignty and governance. Why data sovereignty matters Principles of Māori Data Sovereignty https://researchscienceinnovation.nz/case-studies/relentless-science-communication-in-the-time-of-covid/index.html↩︎ "],["introduction-to-the-design-and-analysis-of-experiments.html", "Chapter 4 Introduction to the design and analysis of experiments 4.1 Introduction to experiments 4.2 The three (main) principles of experimental design 4.3 Experimental designs", " Chapter 4 Introduction to the design and analysis of experiments Learning objectives Identify the following in a given experiment experimental unit observational units treatment(s) List and describe the three main principles of experimental design, specifically, randomization replication blocking Describe the layout and set-up of a CRD, RCBD, and a simple factorial experimental design Discuss and critique a given experimental design Identify sources of variation within a given experimental design We now know how to deal with data in R; however, before we start drawing conclusions we need to know how the data were collected. Generally, data is either observational (data collected where researchers don’t control the environment, but simply observe outcomes) or experimental (data collected where researchers introduce some intervention and control the environment in order to draw inference). Being able to design and critique appropriate experiments is a key skill for any budding biologist. But how do you start? R. A. Fisher’s work in the area of experimental design is, perhaps, the most well known in the field. The principles he devised we still abide by today. Note, however, to give a balanced view of the celebrated mathematician many of his views (on eugenics and race in particular) are detested to many. I would urge you to read up on his work in this area and come to your own conclusions. 4.1 Introduction to experiments “A useful property of a test of significance is that it exerts a sobering influence on the type of experimenter who jumps to conclusions on scanty data, and who might otherwise try to make everyone excited about some sensational treatment effect that can well be ascribed to the ordinary variation in [their] experiment.” — Gertrude Mary Cox Key phrases An experiment is a procedure (or set of actions) where a researcher intentionally changes some factor/treatment/variable to observe the effect of their actions. As mentioned above, the collection of observational data is not experimentation. An experimental unit is the smallest portion of experimental material which is independently perturbed. This is the item under study for which some variable (treatment) is changed. For example this could be a human subject or an agricultural plot. An observational unit (or subsample) is the smallest unit on which a response is measured. If the experimental unit is split after the treatment has been applied (e.g., multiple samples taken from one human subject) then this sample is called a subsample or observational unit. If one measurement is made on each experimental unit then the observational unit = the experimental unit. If multiple measurements are made on each subject (e.g., human) then each experimental unit has &gt;1 observational unit. This is then pseudo- or technical replication (see below). A treatment (or independent variable or factor or treatment factor) is an experimental condition independently applied to an experimental unit. It is one of the variables that is controlled by the researcher during the experiment (e.g., drug type). The values of the treatments within a set are called levels. The dependent variable or response is the output (or thing) that is measured after an experiment. This is what the researcher measures and assesses if changing the treatment(s) (i.e., independent variable(s)) induces any change. An effect is the change in the response variable caused by the controlled changes in the independent variable. Whether the magnitude of the effect (it’s size) is significant (or or any practical interest) is determined by the researcher after carrying out some appropriate analyses. 4.1.1 Setting up an experiment In the previous chapter where we discussed Reproducibility. This also applies to the design and analysis of experiments. In order to future proof our research we should make every effort to ensure others can reproduce it. To do so we should be specific about our goals and procedures by Defining the goals/objectives of our research, Formulating a specific hypothesis, Specifying the response variable(s) that will be measured, Specifying the treatments that will be tested and describing the process of applying these treatments to the experimental units, Outlining the procedure for observing and recording responses to assess treatment effects, Identifying factors that may contribute to variability in the results (expected and otherwise), and Describing the statistical or methods that will be employed to the our hypothesis. Following these guidelines not only helps our experiment, but makes sure others can reproduce it. For example, defining specific objectives directs you towards writing focused statements about the investigative questions you want your experiment to answer. Listing the experimental factors (or treatments or independent variables) you will study in your experiment helps to organize variables and work out how they may help to explain observed changes in your measurable response(s). It is important that the experimental factor can be controlled during and between experimental runs. Variables that are thought to affect the response, but cannot be controlled, cannot be included as an experimental factor. An example Experiment Title Barista Brews Researcher Name Dr Java Researcher Institute Central Perk Objective The objective of this experiment is to determine what type of grind and brew ratio (i.e., amount of coffee in relation to water) leads to the strongest coffee. Hypothesis That a finer grained coffee with a higher brew ratio (i.e., less water to coffee) will lead to the strongest coffee. Response variable Total Dissolved Solids (TDS), a measure of soluble concentration (%). Treatments - Brew ratio - 2 parts coffee to 1 part water (2-1) - 1 part coffee to 1 part water (1-1) - 1 part coffee to 2 parts water (1-2) - Grind - Fine - Coarse Experimental material Individual cups, maintaining consistency in cup size, water amount, and boiling temperature. How treatments will be applied Independently to individual cups. Each cup will be made independently, not as a batch distributed among several cups. How measurements will be taken Treatments applied independently to individual cups. Each cup with the same treatment (brew ratio and grind) will be subject to the same environmental conditions. Experimental units Replicate each treatment twice. Assign 2 experimental units (cups) to each unique treatment combination. TASK Design your own experiment briefly outlining each of the steps listed above. You may find this application useful. 4.2 The three (main) principles of experimental design Replication Repeating an experiment means that you can assess the consistency and reliability of any observed effects. If you observe the effect repeatedly is less likely that it occurred simply due to random variation. There are many ways replication can be included in an experiment. What you repeat (e.g., treatments or measurements) depends on what effect or source of variation you wish to investigate. The table below summarizes, perhaps, the three most commonly employed types of replication. Replication Type Description Why Biological Replication Each treatment is independently applied to several humans, animals, or plants. To generalize results to the population. Technical Replication Two or more samples from the same biological source independently processed. Advantageous if processing steps introduce a lot of variation; increases precision in comparing relative abundances between treatments. Pseudo-replication One sample from the same biological source divided into two or more aliquots independently measured. Advantageous for noisy measuring instruments; increases precision in comparing relative abundances between treatments. Randomization Employing randomization in your experiment goes towards ensuring the validity, reliability, and generalizability of your results. The main reason to randomize allocation of treatment to experimental units is to protect against bias. We, typically, wish to plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of “chance”. Doing so ensures that each treatment has the same probability of getting good (or bad) units and thus avoids systematic bias. Random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups. Hence, by creating comparable treatment groups through random assignment we can minimize bias, increase validity and genralizability of the results. Typically statistical analysis assumes that observations are independent. This is almost never strictly true in practice but randomization means that our estimates will behave as if they were based on independent observations. Randomization is also useful in situations where we may be are unaware of all potential variables as it is more likely to distribute the effects of unknown factors evenly. In addition, randomizing treatment allocation can aid in the ethical conduct of our research as it promotes fair distribution of benefits and risks among participants. Blocking Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar (homogeneous) than units from different subsets or blocks. The experiment is then conducted separately within each block. Blocking is a technique for dealing with nuisance factors, a factor that has some effect on the response, but is of no interest. By grouping similar units together in blocks we reduce the effect of the nuisance factors, which can improve the precision of our estimates (magnitude of effects). In addition, blocking helps prevent confounding by controlling the impact of known or suspected sources of variation. Let’s briefly delve into the realms of fantasy3. The fantastical example we consider is set at Basgiath College where students are trained to serve the Empire. Students are either trained as Riders or Scribes entering the respective Quadrant. Along comes a new Professor, Professor Llewelyn, who thinks that their new dragon-riding coaching technique is guaranteed to result in huge improvements in a student’s dragon-riding ability. They conduct an experiment comparing the new and old coaching techniques. To conduct their experiment they use a mix of students from the two different Quadrants: Riders (who have been riding dragons all their lives) and Scribes (who have never ridden a dragon before). Before any coaching is given all students are asked to complete a dragon assault course whilst atop a dragon, which is timed. Then, over a few months, each student randomly receives one of the coaching techniques (i.e., new or old). After this, the students are asked to complete another assault course of similar difficulty and their time is again recorded. Their improvement (if any) in dragon riding is measured by the difference between the two times. However, the huge variation in baseline riding ability between these two groups (i.e., Riders are obviously going to be much better at riding dragons than Scribes) will obfuscate/obscure any improvement the new teaching technique may induce. Therefore, to find out if the new dragon-riding coaching technique works Professor Llewelyn blocks by Quadrant setting up the experiment as follows. Title Objective of experiment Examine the impact of a new dragon riding coaching technique Primary Variable of Interest Improvement in dragon riding (difference between assult course completion times) Nuisance Factors Introducing Variability Quadrant (i.e., Riders or Scribes) Blocking We use blocking based on Quadrants to control for variability introduced by the differences among Riders and Scribes Experimental Units Students are the experimental units Blocking Procedure 1. Identify Quadrant of student (i.e., Rider or Scribe). 2. Create Blocks: Group students based on Quadrant. 3. Random Assignment within Blocks: Randomly assign students to different coaching techniques (i.e., old and new). This ensures each technique (e.g., old and new) is tested with different Quadrant. Experimental Setup Each block (representing a Quadrant) contains a random assignment of students receiving different dragon riding coaching. Benefits of Blocking By using blocking based on Quadrant, the experiment controls for potential variations dragon riding improvement caused by the baseline differences in a student’s dragon riding ability due to their Quadrant. This enhances the precision of the experiment and allows for a more accurate assessment of the impact of the coaching on dragon riding skill. Example Outcome After the experiment, researchers analyze dragon riding improvement separately for each Quadrant, drawing conclusions about the technique’s effectiveness within specific Quadrants while minimizing the influence of Quadrant on the overall results. TASK Identify the observational units, treatment levels, response variable, and the number of blocks in the experiment above. 4.3 Experimental designs 4.3.1 Completely randomised design (CRD) Let’s consider a completely randomized design with one treatment factor (e.g., coffee bean type). Here, \\(n\\) experimental units (e.g., cups) are divided randomly into \\(t\\) groups. Random allocation can be achieved by simply drawing lots from a hat! To be more rigorous, though, we could use R’s sample() function (have a go yourself and see if you can work out how to wield sample()). Each group is then given one treatment level (one of the treatment factors). As we have defined only one treatment factor all other known independent variables are kept constant so as to not bias any effects. An illustration of a CRD with one tratment factor and three treatment levels (A, B, &amp; C) Designing a CRD using R Here we’re going to use R to do the random allocation of treatments for us. ## create a character vector of bean types beans &lt;- rep(c(&quot;Arabica&quot;,&quot;Liberica&quot;, &quot;Robusta&quot;), each = 4) beans ## [1] &quot;Arabica&quot; &quot;Arabica&quot; &quot;Arabica&quot; &quot;Arabica&quot; &quot;Liberica&quot; &quot;Liberica&quot; ## [7] &quot;Liberica&quot; &quot;Liberica&quot; &quot;Robusta&quot; &quot;Robusta&quot; &quot;Robusta&quot; &quot;Robusta&quot; ## randomly sample the character vector to give the order of coffees set.seed(1234) ## this is ONLY for consistency, remove if doing this yourself allocation &lt;- sample(beans, 12) allocation ## [1] &quot;Robusta&quot; &quot;Robusta&quot; &quot;Liberica&quot; &quot;Liberica&quot; &quot;Arabica&quot; &quot;Liberica&quot; ## [7] &quot;Arabica&quot; &quot;Robusta&quot; &quot;Arabica&quot; &quot;Liberica&quot; &quot;Robusta&quot; &quot;Arabica&quot; Having run the code above your CRD plan is as follows Cup 1 2 3 4 5 6 7 8 9 10 11 12 Bean Robusta Robusta Liberica Liberica Arabica Liberica Arabica Robusta Arabica Liberica Robusta Arabica TASK Run the R code above, but this time choose a different random seed by choosing a different number in this line of code set.seed(1234). What is your CRD plan? Why might keeping the random seed be important? 4.3.2 Randomised complete block design (RCBD) Let’s consider a randomized complete block design with one treatment factor (e.g., coffee bean type). If the treatment factor has \\(t\\) levels there will be \\(b\\) blocks that each contain \\(t\\) experimental units resulting in a total of \\(t\\times b\\) experimental units. For example, let’s imagine that for the coffee experiment we had two cup types: mugs and heatproof glasses. We might consider the type of receptacle to have an effect on the coffee strength measured, however, we are not interested in this. Therefore, to negate this we block by cup type. This means that any effect due to the blocking factor (cup type) is accounted for by the blocking. For a blocked design we want the \\(t\\) experimental units within each block should be as homogeneous as possible (as similar as possible, so that there is unlikely to be unwanted variation coming into the experiment this way). The variation between blocks (the groups of experimental units) should be large enough (i.e., blocking factors different enough) so that conclusions can be drawn. Allocation of treatments to experimental units is done randomly (i.e., treatments are randomly assigned to units) within each block. An illustration of a CRD with one tratment factor, three treatment levels (A, B, &amp; C), and three blocks (rows) Designing a RCBD using R Let’s assume you want to set up an experiment similar to the CRD one above; however, now you are in the situation where you have two types of cups (mugs and heatproof glasses). Below we use R to do the random allocation of treatments within each block (cup type) for you. Here we have \\(t = 3\\) treatments (bean types) and \\(b = 2\\) blocks (cup types) so we will have \\(t \\times b = 6\\) experimental units in total. set.seed(4321) ## this is ONLY for consistency, remove if doing this yourself plan &lt;- data.frame(Beans = rep(c(&quot;Arabica&quot;,&quot;Liberica&quot;, &quot;Robusta&quot;), times = 2), Block = rep(c(&quot;Mug&quot;, &quot;Glass&quot;), each = 3)) %&gt;% ## combine experiment variables group_by(Block) %&gt;% ## group by blocking factor dplyr::sample_n(3) plan ## # A tibble: 6 × 2 ## # Groups: Block [2] ## Beans Block ## &lt;chr&gt; &lt;chr&gt; ## 1 Arabica Glass ## 2 Liberica Glass ## 3 Robusta Glass ## 4 Liberica Mug ## 5 Arabica Mug ## 6 Robusta Mug Having run the code above your RCBD plan is as follows Receptacle 1 2 3 1 2 3 Beans Arabica Liberica Robusta Liberica Arabica Robusta Block Glass Glass Glass Mug Mug Mug TASK Run the R code above, but this time choose a different random seed by choosing a different number in this line of code set.seed(4321). What is your CRD plan? Why might keeping the random seed be important? 4.3.3 Factorial design A factorial experiment is one where there are two or more sets of (factor) treatments4. Rather than studying each factor separately all combinations of the treatment factors are considered. Factorial designs enable us to infer any interaction effects, which may be present. An interaction effect is one where the effect of one variable depends on the value of another variable (i.e., the effect of one treatment factor on the response variable will change depending on the value of a second treatment factor.) Let’s consider Dr Java’s experiment above. Here, Dr Java wants to study all combinations of the levels of each factor. Their objective is to determine what type of grind and brew ratio (i.e., amount of coffee in relation to water) leads to the strongest coffee. Possible outcomes 4.3.3.1 Interactions If an interaction effect exists the effect of one factor on the response will change depending on the level of the other factor. The plot above is called an interaction plot. Creating such a plot is often very useful when drawing inference; in this instance we can see that the strength of the coffee changes depending on the type of coffee bean used, however, this relationship differs depending on the type of grind used. For example, Liberica beans produce stronger coffee than the other two beans when the fine grind is used, but weaker coffee when the coarse grind is used. Designing factorial experiment using R Here we use the R package edibble which is specifically designed to aid in the planning and designing of experiments. library(edibble) ## you will need to install this package first ## Factorial design (Brew and Grind) design(&quot;Barista Brew: Factorial Design&quot;) %&gt;% set_units(cup = 12) %&gt;% # 2*2*3 set_trts(BrewRatio = c(&quot;2-1&quot;, &quot;1-1&quot;, &quot;1-2&quot;), Grind = c(&quot;Fine&quot;, &quot;Coarse&quot;)) %&gt;% allot_trts(~cup) %&gt;% assign_trts(&quot;random&quot;, seed = 836) %&gt;% ## the random seed for random allocation serve_table() ## # Barista Brew: Factorial Design ## # An edibble: 12 x 3 ## cup BrewRatio Grind ## &lt;U(12)&gt; &lt;T(3)&gt; &lt;T(2)&gt; ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 cup01 1-2 Fine ## 2 cup02 2-1 Coarse ## 3 cup03 1-1 Fine ## 4 cup04 2-1 Coarse ## 5 cup05 1-2 Coarse ## 6 cup06 1-1 Coarse ## 7 cup07 1-2 Fine ## 8 cup08 1-1 Fine ## 9 cup09 1-2 Coarse ## 10 cup10 1-1 Coarse ## 11 cup11 2-1 Fine ## 12 cup12 2-1 Fine TASK Run the R code above, but this time choose a different random seed (by changing seed = 836). What is your design? Now increase the number of replications, use your R skills to figure out what line of code needs changing here. What is your design now? Other resources: optional but recommended Glass, David J. Experimental Design for Biologists. Second ed. 2014. Print. Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print. Fisher, Ronald Aylmer. The Design of Experiments. 8th ed. Edinburgh: Oliver &amp; Boyd, 1966. Print. O &amp; B Paperbacks. Lawson, John. Design and Analysis of Experiments with R. Vol. 115. CRC press, 2014. A realm based on that in the Fourth Wing.↩︎ Note is a factorial design has equal numbers of replicates in each group then it is said to be a balanced design; if this is not the case then it is unbalanced.↩︎ "],["introduction-to-hypothesis-testing.html", "Chapter 5 Introduction to Hypothesis testing 5.1 Randomization tests 5.2 What is a p-value? 5.3 The Vocabulary of hypothesis testing", " Chapter 5 Introduction to Hypothesis testing Learning objectives Formulate a question/hypothesis to investigate based on the given data Write out the appropriate null and alternative hypothesis using statistical notation for a randomization test (given a statistic) Given a statistic write R code to carry out a randomization test Correctly interpret and communicate a p-value in terms of a randomization test Define the meaning of false positive (Type I error) and false negative (Type II error) effects in terms of a hypothesis test State in terms of probability statements the meaning of the power and significance level of an hypothesis test We saw in the previous chapter (in Setting up an experiment) that a hypothesis often serves as the foundation for designing experiments and therefore collecting data. A hypothesis should be specific and testable (i.e., we should be able to collect evidence either in support of or against the hypothesis). Hypotheses should mention measurable variables; typically a dependent/response variable (the outcome being measured) and one or more independent/treatment variables (being manipulated or studied). Once we have specified our hypothesis we want to test it! That is find evidence for or against it. To do so reliably we need to use rigorous statistical methodology. In this section you will be (re)introduced5 to a straightforward, yet hugely powerful statistical tool: randomization tests! 5.1 Randomization tests The basic approach to a randomization test is straightforward: Choose a statistic to measure the effect in question (e.g., differences between group means) Calculate that test statistic on the observed data. Note this metric can be anything you wish Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under the Null hypothesis, \\(H_0\\)): For a chosen number of times shuffle the data labels calculate the test statistic for the reshuffled data and retain Find the location of your observed statistic in the sampling distribution. The location of observed statistic in sampling distribution is informative: if in the main body of the distribution then the observed statistic could easily have occurred by chance if in the tail of the distribution then the observed statistic would rarely occur by chance and there is evidence that something other than chance is operating. Calculate the proportion of times your reshuffled statistics equal or exceed the observed. This p-value is the probability that we observe a statistic at least as “extreme” as the one we observed State the strength of evidence against the null on the basis of this probability. TASK Briefly outline what effect your choice of a chosen number of times has on your randomization test. 5.1.1 A randomization test for two independent samples Remember the Pāua data from earlier in the module? This dataset contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) We can read these data directly into R using the URL and the readr package, which is part of the tidyverse collection) as follows. paua &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv&quot;) glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;… ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.… ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6… One question we may want to ask is if, on average, the shell length differs between Species? We would formulate this as a hypothesis test as follows Null hypothesis: The mean length of Haliotis iris shells is the same as Haliotis australis Alternative hypothesis: The mean length of Haliotis iris shells is not the same as Haliotis australis Test statistic: Difference of sample means Now, statisticians love using Greek letters to symbolize things. Typically, the letter \\(\\mu\\) is used to represent a mean. So, by letting \\(\\mu_\\text{Haliotis iris}\\) represent the average shell length of Haliotis iris and \\(\\mu_\\text{Haliotis australis}\\) represent the average shell length of Haliotis australis we could synthase this hypothesis test as Null hypothesis: \\(\\mu_\\text{Haliotis iris} = \\mu_\\text{Haliotis australis}\\) Alternative hypothesis: \\(\\mu_\\text{Haliotis iris} \\neq \\mu_\\text{Haliotis australis}\\) Test statistic: \\(\\mu_\\text{Haliotis australis} - \\mu_\\text{Haliotis iris}\\) (or \\(\\mu_\\text{Haliotis iris} - \\mu_\\text{Haliotis australis}\\) it doesn’t matter here) Finally, not having had enough of mathematical syntax quite yet by convention a Null hypothesis is denoted \\(\\text{H}_0\\) and an alternative hypothesis by \\(\\text{H}_1\\). So our hypothesis test can finally be written concisely as \\(\\text{H}_0:\\) \\(\\mu_\\text{Haliotis iris} = \\mu_\\text{Haliotis australis}\\) vs \\(\\text{H}_1:\\) \\(\\mu_\\text{Haliotis iris} \\neq \\mu_\\text{Haliotis australis}\\) It follows from this definition that out test statistic is \\(\\mu_\\text{Haliotis australis} - \\mu_\\text{Haliotis iris}\\). The vs is convention as we are considering two opposing hypotheses. Having, defined the hypothesis test above what would you expect the answer to be looking at the plot below? 1. Choose a statistic that measures the effect of interest (in this case the differences between means) ## observed differences in means diff_in_means &lt;- paua %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Length)) %&gt;% summarise(diff = diff(mean)) %&gt;% as.numeric() diff_in_means ## [1] -0.9569444 2. Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under \\(H_0\\)) ## Number of times I want to randomize nreps &lt;- 9999 ## initialize empty array to hold results randomisation_difference_mean &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## the observations data &lt;- data.frame(value = paua$Length) ## randomize labels data$random_labels &lt;-sample(paua$Species, replace = FALSE) ## randomized differences in mean randomisation_difference_mean[i] &lt;- data %&gt;% group_by(random_labels) %&gt;% summarise(mean = mean(value)) %&gt;% summarise(diff = diff(mean)) %&gt;% as.numeric() } ## results, combine with the observed statistic results &lt;- data.frame(randomisation_difference_mean = c(diff_in_means, randomisation_difference_mean)) 3. Locate the observed statistic (i.e., from our observed random sample) in the sampling distribution The location of observed statistic in the sampling distribution allows us to draw inference with regards to our hypothesis. If the observed statistic is in the main body of the distribution then it is more than likely that the observed statistic occurred by chance. However, if the observed statistic is in the tail of the distribution then it would rarely occur by chance and we have evidence against the observed statistic occurring by chance alone. ## How many randomized differences in means are as least as extreme as the one we observed ## absolute value as dealing with two tailed n_exceed &lt;- sum(abs(results$randomisation_difference_mean) &gt;= abs(diff_in_means)) n_exceed ## [1] 18 We see something as least as extreme as our observed test statistic 18 times. This translates to a two-sided tail proportion of 0.018. The plot below illustrates the location of the observed test statistic in that of the distribution of the test statistics we calculated under the null hypothesis. In experimental situations a large p-value (large tail proportion) means that the luck of the randomization quite often produces group differences as large or even larger than what we’ve got in our data. A small p-value means that the luck of the randomization draw hardly ever produces group differences as large as we’ve got in our data. So what do you conclude here? NOTE: We can extend the randomization test to make inference about any sample statistic (not just the mean) TASK Run through the code chunks above but change your code accordingly so that your hypothesis test is now Null hypothesis: On average, the Age of Haliotis iris is the same as Haliotis australis Alternative hypothesis: On average, the Age of Haliotis iris is not the same as Haliotis australis Test statistic: Difference of sample means What do you conclude? TASK Run through the code chunks above but change your code accordingly so that your hypothesis test is now Null hypothesis: The median length of Haliotis iris shells is the same as Haliotis australis Alternative hypothesis: The median length of Haliotis iris shells is not the same as Haliotis australis Test statistic: Difference of sample medians What do you conclude? 5.2 What is a p-value? “Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.” — ASA Statement on p-Values Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. There are many different schools of thought about how a p-value should be interpreted. Most people agree that the p-value is a useful measure of the strength of evidence against the null hypothesis. The smaller the p-value, the stronger the evidence against \\(H_0\\). Some people go further and use an accept/reject framework. Under this framework, the null hypothesis \\(H_0\\) should be rejected if the p-value is less than 0.05 (say), and accepted if the p-value is greater than 0.05. In this course we mostly use the strength of evidence interpretation. The p-value measures how far out our observation lies in the tails of the distribution specified by \\(H_0\\): In summary, p-values can indicate how incompatible the data are with a specified statistical model; they do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Note that a p-value does not measure the size of an effect or the importance of a result and by itself it does not provide a good measure of evidence regarding a model or hypothesis. Note also, that a substantial evidence of a difference does not equate to evidence of a substantial difference! Any scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold as proper inference requires full reporting and transparency. Remember that statistical significance does not imply practical significance, and that statistical significance says nothing about the size of treatment differences. To estimate the sizes of differences you need confidence intervals, look out for these in the following chapters. Some p-value threshold recursive FAQs Question Answer Why do so many colleges and grad schools teach p-val=0.05? Because that’s still what the scientific community and journal editors use. BUT IT SHOULDN’T BE Why do so many people still use p-val=0.05? | Because that’s what they were rote taught in college or grad school. BUT THEY SHOULDN’T BE | TASK Briefly outline why a threshold cutoff is problematic in the context of hypothesis testing 5.3 The Vocabulary of hypothesis testing “Type I Zoom error: you think people can hear you, but you’re actually on mute. Type II Zoom error: you think your muted, but actually people can hear you.” — @CynPeacock, Twitter State of Nature Don’t reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true Type I error \\(H_0\\) is false Type II error Type I errors A Type I error (false positive): declare a difference (i.e., reject \\(H_0\\)) when there is no difference (i.e. \\(H_0\\) is true). Risk of the Type I error is determined by the level of significance (which we set!) (i.e., \\(\\alpha =\\text{ P(Type I error)} = \\text{P(false positive)}\\). Type II A Type II error (false negative): difference not declared (i.e., \\(H_0\\) not rejected) when there is a difference (i.e., \\(H_0\\) is false). Let \\(\\beta =\\) P(do not reject \\(H_0\\) when \\(H_0\\) is false); so, \\(1-\\beta\\) = P(reject \\(H_0\\) when \\(H_0\\) is false) = P(a true positive), which is the statistical power of the test. Significance level The significance level is the probability of a Type I error (i.e., the probability of finding an effect that is not there, a false positive). Power The power of a test is the probability that the test correctly rejects the null hypothesis when the alternative hypothesis is true. The probability of finding an effect that is there = 1 - probability of a Type II error (false negative). Reducing the chance of a Type I error increases the chance of a Type II error. They are inversely related. Type II error rate is determined by a combination of the following. Effect size (size of difference, of biological significance) between the true population parameters Experimental error variance Sample size Choice of Type I error rate (\\(\\alpha\\)) Family-Wise Error Rate (FWER) Each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). Other resources: optional but recommended Cheat sheet: randomisation explained by cows… The ASA Statement on p-Values: Context, Process, and Purpose if you did STATS10X then you have already covered randomization tests.↩︎ "],["hypothesis-testing-and-introduction-to-linear-regression.html", "Chapter 6 Hypothesis testing and introduction to linear regression 6.1 Hypothesis testing 6.2 Model assumptions 6.3 Model diagnostic plots 6.4 Other resources: optional but recommended", " Chapter 6 Hypothesis testing and introduction to linear regression Learning outcomes Describe the difference between a scientific hypothesis and statistical hypotheses (H0 and H1/HA) Apply the four steps of hypothesis testing and write R code for one-sample t-test two-sample independent t-test one-way Analysis of Variance (ANOVA) Identify similarities of hypothesis tests listed above to simple linear regression List linear regression model assumptions and be able to evaluate model diagnostics to assess model suitability 6.1 Hypothesis testing 6.1.1 Scientific vs statistical hypotheses Scientific/Research hypotheses are expected or predicted relationships between two or more variables that motivate a piece of research. They stem from the research question and often are developed from prior knowledge or theoretical expectation. Statistical hypotheses involve restating the research hypothesis so it can be addressed by statistical techniques, such as hypothesis testing. For hypothesis testing, two statistical hypotheses are presented: Null hypothesis H0 It is the “no news here” hypothesis. It is always specific; it identifies one particular value for the parameter of interest. Usually it is there is no difference, no relationship, no effect (i.e., usually = 0). Most often it would be interesting to find evidence against the null hypothesis. This the only hypothesis that is actually being tested when we conduct a hypothesis test. Alternative hypothesis H1/HA It is the “new news” hypothesis. It is the opposite of the null, often everything that isn’t the null hypothesis. There is a difference, a relationship, an effect (i.e. usually \\(\\neq\\) 0). The alternative contains possibilities that would be more biologically interesting than what is stated in the null hypothesis. The alternative also often contains values that are predicted in the scientific hypothesis being evaluated by the statistical test, so the researcher often hopes to find evidence against the null hypothesis. 6.1.2 Four basic steps of hypothesis testing 1. State the hypotheses 2. Compute the test statistic 3. Determine the p-value 4. Draw the appropriate conclusions Actually, there is a couple of initial things to do before step 1. These are to determine what type of statistical test you will conduct (based off what metric you wish to measure and your scientific hypothesis and data - see lecture and Hypothesis testing H5P for more information) and check to make sure the assumptions of said test are met. If assumptions are not met, you’ll need to adjust the test you conduct. We’ll go over model assumptions and model diagnostics in lecture and later in this guide. We’ll go through all the steps of hypothesis testing by going through some worked examples. We will, again, use the paua.csv data, available from CANVAS and earlier in this course guide. Recall that the P\\(\\overline{\\text{a}}\\)ua dataset contains the following variables: Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimetres Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (Blackfoot p\\(\\overline{\\text{a}}\\)ua) and Haliotis australis (Yellowfoot p\\(\\overline{\\text{a}}\\)ua) - both endemic to NZ library(tidyverse) paua &lt;- read_csv(&quot;paua.csv&quot;) It is a good idea to check our data set has imported into R correctly and is how we expect it to look glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;… ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.… ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6… 6.1.3 One-Sample t-test Using a violin plot we can look at the distribution of shell Length (i.e., one variable hence a one-sample t-test will be used). We can calculate the average Length of all shells in our sample. paua %&gt;% summarise(average_length = mean(Length)) ## # A tibble: 1 × 1 ## average_length ## &lt;dbl&gt; ## 1 5.19 What about drawing inference? Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is, say, 5cm (perhaps some previous research suggests this value)? This is our research question. Let’s turn this into statistical hypotheses that we can test. 6.1.3.1 Step one. State the hypotheses Null hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are 5 cm long Alternative hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are not 5 cm long Notationally: \\(H_0: \\mu = 5\\) vs \\(H_1: \\mu \\neq 5\\) (\\(\\mu\\) is the proposed population mean) 6.1.3.2 Step two. Compute the test statistic We know our sample average (5.192 cm), but can we make any claims based on this one number? Other useful information to help us reflect the uncertainty about our sample mean is the spread/variability of data around this mean. Visualising the uncertainty in a one-sample t-test - the SEM The Standard Error of the Mean (SEM) is a measure of uncertainty about the mean. We can visualise this uncertainty by adding error bars around the sample mean on our violin plot. Typically \\(\\pm\\) twice the SEM is the interval used: Why error bars that are \\(\\pm\\) twice the SEM? This is approximately the 95% confidence interval for the population mean. We can use the in-built functions in R to calculate the mean and the SEM. Yay - we don’t have to do these calculations by hand, R can do them for us! For the paua data, here’s some code we can use: sem &lt;- paua %&gt;% summarise(mean = mean(Length), sem = sd(Length)/sqrt(length(Length))) sem ## # A tibble: 1 × 2 ## mean sem ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.19 0.155 Note: have you noticed the font in the code above is coloured differently? The dark blue font indicates an R function to do a calculation, the green font indicates the name of an object being created. Try to change these and see what the headers of the tibble output change to, and Length in black font is the variable in the paua data set that we want to summarise. (you will not see the colours if you are reading this course guide printed in black and white). The test statistic is a way of summarising the sample mean and its uncertainty into a single number. For a t-test, the test statistic is the t-statistic or the t-value. The t-statistic for a one-sample t-test is: t-statistic \\(= \\frac{\\bar{x}- \\mu}{\\text{SEM}}\\) = \\(\\frac{5.1925 - 5}{0.155351}\\) = 1.239 \\(\\bar{x}\\) is the sample mean \\(\\mu\\) is the theoretical value (proposed population mean) SEM is the Standard Error of the Mean. SEM \\(= \\frac{\\sigma}{\\sqrt{n}}\\); where \\(\\sigma\\) is a population parameter that we estimate using our sample standard deviation (SD). \\(SD = \\sqrt{\\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) (\\(i = 1,...,n\\)) is the standard deviation (SD) of the sample, \\(n\\) is the sample size, and \\(\\bar{x}\\) is the sample mean. (to learn more about how this is calculated, see “Other resources:optional but recommended” at the end of this section) We want to compare this t-statistic to the null distribution, which is the most likely range of values that will occur if we assume the null hypothesis is true (R can create this null distribution for us, which is a t-distribution for a t-test). This comparison generates a p-value, which is the next step in hypothesis testing. 6.1.3.3 Step three. Determine the p-value Recall that a p-value is the probability under a specified statistical model (here, the null distribution) that a statistical summary of the data would be equal to or more extreme than its observed value (here, the t-statistic). So in this case it’s the probability, under the null hypothesis (\\(\\mu = 5\\)), that we would observe a t-statistic at least as extreme as we did. The null distribution, assuming H0 is true, is below. The t-statistic calculated from our hypothesis test was 1.2391. Now, remember that our alternative hypotheses was \\(H_1: \\mu \\neq 5\\) so we have to consider both sides of the inequality; hence, anything as least as extreme is either \\(&gt; 1.2391\\) or \\(&lt; -1.2391\\) to our observed test statistic (vertical lines). Anything at least as extreme is therefore given by the grey shaded areas. We can calculate the p-value using the pt() function (where q is our calculated t-statistic, and df are the degrees of freedom from above): 2*(1 - pt(q = 1.2391,df = 59)) ## [1] 0.2202152 But why make life hard for ourselves - let’s just do all of the above (specify our hypotheses, compute our test statistic and determine our p-value) in one step using R!! t.test(paua$Length, mu = 5 ) ## ## One Sample t-test ## ## data: paua$Length ## t = 1.2391, df = 59, p-value = 0.2202 ## alternative hypothesis: true mean is not equal to 5 ## 95 percent confidence interval: ## 4.881643 5.503357 ## sample estimates: ## mean of x ## 5.1925 6.1.3.4 Step four. Draw the appropriate conclusions So our p-value - the probability that under our null hypothesis we observe anything as least as extreme as what we did - is ~0.22. Do you think what we’ve observed is likely under the null hypothesis? Or in other words, do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is 5 cm? Big p-values mean what we observed isn’t surprising. A big p-value doesn’t prove that the null hypothesis is true, but it certainly offers no evidence that it is not true. Thus, when we see a large p-value, all we can say is that we have no evidence against the null hypothesis. Does the plot below help to visualise this? The proposed population mean is shown by the solid red horizontal line; the dashed line shows the sample mean and the dotted lines \\(\\pm\\) twice the SEM (i.e., ~95% confidence interval). One of the most important steps is communicating our findings to our audience. When writing up results from hypothesis testing, always include these values listed below so the reader can determine how you obtained your p-value: the value of the test-statistic (here our t-statistic is 1.2391) the sample size (n) (here n = 60 - which R code have we have written in previous weeks to find this out?) the p-value, related back to your null hypothesis (in biological terms) (here our p-value is 0.22 or 22% which means we have no evidence against the null hypothesis that the average population shell length for both species of p\\(\\overline{\\text{a}}\\)ua is 5 cm). Report the actual p-value and not some level of significance. The p-value indicates the weight of evidence against the null hypothesis, but it does not measure the size of the effect, so for best practice also include the confidence interval, also in biological terms (in the t.test output, we can see the values for our confidence interval are 4.882 and 5.503). We will go over confidence intervals in more detail in Chapter 7: Statistical inference. 6.1.4 Two-sample independent t-test (with lm()and t.test() in R) We have two species of p\\(\\overline{\\text{a}}\\)ua in this data set, so it may be more interesting to ask is there a difference in the average shell length of Haliotis australis and Haliotis iris? 6.1.4.1 Step one. State the hypotheses Null hypothesis: On average the species’ shells are the same length Alternative hypothesis: they aren’t the same! Notationally: \\(H_0: \\mu_{\\text{Haliotis australis}} - \\mu_{\\text{Haliotis iris}} = 0\\) OR \\(H_0: \\mu_{\\text{Haliotis australis}} = \\mu_{\\text{Haliotis iris}}\\) vs  \\(H_1: \\mu_{\\text{Haliotis australis}} - \\mu_{\\text{Haliotis iris}} \\neq 0\\) OR \\(H_1: \\mu_{\\text{Haliotis australis}} \\neq \\mu_{\\text{Haliotis iris}}\\) Haliotis australis average - Haliotis iris average = \\(\\bar{x}_{\\text{Haliotis australis}} - \\bar{x}_{\\text{Haliotis iris}}\\) = 5.767 - 4.81 = 0.957. This tells us the difference, but is the difference large enough to say they are different, or is 0.957 similar enough to 0 to say there is no difference? Adding information about the variation around the average values can help! Recall the SEM from the one-sample t-test? The same idea holds here in calculating the test-statistic for a two-sample t-test, although the calculation is a little bit more complicated (as we have to think about the number of observations in each group). But from the two group SEMs we can calculate the Standard Error of the Difference, SED, between two means. 6.1.4.2 Step two &amp; three. Compute the test statistic and determine the p-value The test statistic is also called a t-statistic (it is a t-test after all!) t-statistic = \\(\\frac{\\bar{x}_{\\text{difference}} - \\mu}{\\text{SED}}\\) = \\(\\frac{\\bar{x}_{\\text{difference}} - 0}{\\text{SED}}\\) where \\(\\bar{x}_{\\text{difference}}\\) is the differences between the species` averages. Calculations are a bit trickier here so let’s use R. We have two options (both give us the same information): Option 1. using lm() lm(y ~ x, data = ) is the required information of this R function, where lm stands for linear model, y represents the y/dependent/response/outcome variable and x represents the x/independent/explanatory/predictor variable/s. data = tells R which data set to use to find these y and x variables. t.lm &lt;- lm(Length ~ Species, data = paua) # extracting the estimated population parameters summary(t.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.7666667 0.2278199 25.312396 4.911173e-33 ## SpeciesHaliotis iris -0.9569444 0.2941142 -3.253649 1.902460e-03 So, what are the values/regression coefficients from the lm() output above? lm() will calculate the least squares Estimate for the y-intercept ((Intercept)) and the slope (SpeciesHaliotis iris): Estimate of (Intercept) = the baseline = \\(\\bar{x}_\\text{Haliotis australis}\\) = 5.7666667 Std. Error of (Intercept) = SE of \\(\\bar{x}_\\text{Haliotis australis}\\) = SEM = 0.2278199 Estimate of SpeciesHaliotis iris = \\(\\bar{x}_\\text{Haliotis iris}\\) – \\(\\bar{x}_\\text{Haliotis australis}\\) = -0.9569444 Std. Error of SpeciesHaliotis iris = SE of (\\(\\bar{x}_\\text{Haliotis iris}\\) – \\(\\bar{x}_\\text{Haliotis australis}\\) ) = SED = 0.2941142 Hypotheses being tested The t value and Pr (&gt;|t|) are the t-statistic and p-value for testing the null hypotheses: (Intercept) row: Mean abundance is zero for Haliotis australis (not interested in this really) SpeciesHaliotis iris row: No difference between the population mean shell lengths of Haliotis australis and Haliotis iris By default, R orders variables alphabetically. Sometimes, we don’t mind because there is no logical order to our variables. However, sometimes, we want to specify the variable order (e.g., if we ran a study where we had variables labelled before, during and after, after would come first - weird! Or if we ran an experiment where we had a control and three treatments, labelled A, B and C, it would make the most sense to compare all treatments to the control as the baseline). We can use the mutate function in R to change the variable order: # changing the baseline # it&#39;s the order that makes the difference paua_rl &lt;- paua %&gt;% mutate(Species = fct_relevel(Species, &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;)) c.lm &lt;- lm(Length ~ Species, data = paua_rl) summary(c.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.8097222 0.1860141 25.856756 1.578074e-33 ## SpeciesHaliotis australis 0.9569444 0.2941142 3.253649 1.902460e-03 The values/regression coefficients from the lm() output above Estimate of (Intercept) = the baseline = \\(\\bar{x}_\\text{Haliotis iris}\\) = 4.8097222 Std. Error of (Intercept) = SE of \\(\\bar{x}_\\text{Haliotis iris}\\) = SEM = 0.1860141 Estimate of SpeciesHaliotis australis = \\(\\bar{x}_\\text{Haliotis australis}\\) – \\(\\bar{x}_\\text{Haliotis iris}\\) = 0.9569444 Std. Error of SpeciesHaliotis australis = SE of (\\(\\bar{x}_\\text{Haliotis australis}\\) – \\(\\bar{x}_\\text{Haliotis iris}\\) ) = SED = 0.2941142 Hypotheses being tested The t value and Pr (&gt;|t|) are the t-statistic and p-value for testing the null hypotheses: (Intercept) row: Mean abundance is zero for Haliotis iris (not interested in this really) SpeciesHaliotis australis row: No difference between the population mean lengths of Haliotis australis and Haliotis iris Option 2. using t.test() test &lt;- t.test(Length ~ Species, data = paua) # print out the result test ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means between group Haliotis australis and group Haliotis iris is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 # you can print out individual pieces of information from the results using # the $ to subset the data, as below test$statistic ## t ## 3.540364 test$p.value ## [1] 0.0007956853 Listed are the t-statistic, t = 3.5403636 and the p-value, p-value = 0.0007956853 for the hypothesis test outlined above. 6.1.4.3 Step four. Draw the appropriate conclusions From the lm() we got a t-statistic for the hypothesis test we are interested in of -3.253649 (or 3.253649, doesn’t matter which we look at because it is a two-tailed test) and a p-value of 0.001902460 (this is the same number as 1.902460e-03). From the t.test we got a t-statistic of 3.540364 and a p-value of 0.0007956853 Why do these numbers differ? t.test defaults to the Welch t-test, which is used when the variance of two samples is not equal. If you want to see the same results from t.test and lm() try this code out: test_eq_var &lt;- t.test(Length ~ Species, var.equal = TRUE, data = paua) test_eq_var However, we did not use this var.equal = TRUE argument because the variances (the spread of the data) do not really look equal - check out the violin plot! We need to check our data meet model assumptions - this is coming up after we look at One way Analysis of Variance. 6.1.5 One way ANOVA (with lm() and aov() in R) Remember the Palmer penguins? You might find this application useful, now and later… Below is the bill depth (mm) values for each bird in the sample, grouped together by species Now we have more than two groups: \\(3\\) potential comparisons we might be interested in. Remember that each time we carry out a hypothesis test the probability we get a false positive result (Type I error) is given by \\(\\alpha\\) (the level of significance we choose). In light of this we should control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). ANalysis Of VAriance (ANOVA) can do this! 6.1.5.1 Step one. State the hypotheses Null hypothesis: The mean bill depth of each penguin species is the same Alternative hypothesis: The mean bill depth of at least one species differs from the others Notationally: \\(H_0: \\mu_{Adelie} = \\mu_{Chinstrap} = \\mu_{Gentoo}\\) vs \\(H_A:\\) at least one \\(\\mu_{i}\\) is different from the others 6.1.5.2 Step two &amp; three. Compute the test statistic and determine the p-value The test statistic (the single number summary of the sample data) for ANOVA is the F-statistic or the F-value. Consider the ratio below, this is the F-statistic (check out the R output below to see where the values came from for this penguin data example):  \\({\\frac {{\\text{variation due to groups}}}{{\\text{unexplained variance}}}} = {\\frac {{\\text{ mean between-group variability}}}{{\\text{mean within-group variability}}}}\\) \\(=\\frac{\\text{MSB}}{\\text{MSW}}\\) = \\(\\frac{435.39}{1.26}\\) = \\(344.83\\) Option 1. using lm() then anova() # first we need to specify a linear model from the data fit.lm &lt;- lm(bill_depth_mm ~ species, data = penguins_nafree) ## then we apply anova() to our fitted data anova(fit.lm) ## Analysis of Variance Table ## ## Response: bill_depth_mm ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 870.79 435.39 344.83 &lt; 2.2e-16 *** ## Residuals 330 416.67 1.26 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Option 2. using aov() # using aov() aov &lt;- aov(bill_depth_mm ~ species, data = penguins_nafree) summary(aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 870.8 435.4 344.8 &lt;2e-16 *** ## Residuals 330 416.7 1.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.1.5.3 Step four. Draw the appropriate conclusions What do you notice about the results using lm() then anova() or using aov()? They are BOTH THE SAME The probability of getting an F-statistic at least as extreme as the one we observe: p-value Pr(&gt;F) = &lt; 2.2e-16 tells us we have extremely strong evidence against \\(H_0\\). This means it is likely that the population mean bill depth from at least one species differs from the others. Generally, It would be useful to know which groups differ from each other, but our results do not tell us this. To see which groups differ, we can use the regression coefficient output below. Note: this is only an option if first creating a linear model using lm(), hence in this course we recommend this for running an ANOVA. Note: species Adelie will be the baseline in this linear regression with a categorical factor variable by default because it starts with the letter A summary(fit.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.34726027 0.09299608 197.2906740 0.000000e+00 ## speciesChinstrap 0.07332796 0.16497460 0.4444803 6.569867e-01 ## speciesGentoo -3.35062162 0.13877592 -24.1441135 6.622121e-75 The values/regression coefficients from the lm() output above Estimate of (Intercept) = \\(\\text{mean}_{\\text{Adelie}}\\) = 18.3472603 Std. Error of (Intercept) = SE of \\(\\text{mean}_{\\text{Adelie}}\\) = SEM = 0.0929961 Estimate of speciesChinstrap = \\(\\text{mean}_{\\text{Chinstrap}}\\) - \\(\\text{mean}_{\\text{Adelie}}\\) = 0.073328 This means the average bill depth of Chinstrap penguins is estimated to be 0.073328 larger than the average bill depth of Adelie penguins = 18.3472603 + 0.073328 = 18.4205882 mm. Std. Error of speciesChinstrap = SE of (\\(\\text{mean}_{\\text{Chinstrap}}\\) - \\(\\text{mean}_{\\text{Adelie}}\\) ) = SED = 0.1649746 What is \\(\\text{mean}_{\\text{Gentoo}}\\) - \\(\\text{mean}_{\\text{Adelie}}\\)? Hypotheses being tested The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses (Intercept)row: Mean abundance is zero for Adelie population speciesChinstrap row: No difference between the population mean bill depth of Chinstrap and Adelie speciesGentoo row: No difference between the population mean bill depth of Gentoo and Adelie We’re interested in 2 and 3, but not necessarily 1! So, what do you conclude? Does your inference match the plot? Remember when writing up your results to include all relevant information (see section 5.2.4.3) so the reader can determine how you obtained your p-value. 6.2 Model assumptions To make meaningful interpretations of any linear regression outputs, our data must meet some key assumptions Independence There is a linear relationship between the response and the explanatory variable(s) The residuals have constant variance The residuals are normally distributed We can check these assumptions in lots of different ways. In this course, we are using diagnostic plots. 6.3 Model diagnostic plots Again, R is making our lives easy! Such a small amount of code to check our model assumptions - nice one :-) gglm::gglm(fit.lm) # Plot the four main linear regression diagnostic plots 6.3.1 Residuals vs Fitted plot The residuals having constant variance assumption can be checked with this plot. You are looking for a roughly symmetric cloud of points above and below the horizontal line at zero. You want to see no pattern or structure in your residuals (e.g., a “starry” night). You definitely don’t want to see the scatter increasing or decreasing around the zero line as the fitted values get bigger (e.g., trumpet/ wedge of cheese/ slice of pizza shaped) which would indicate unequal variances (heteroscedasticity). [Note: in the residual vs fitted plot above things look a bit weird, the points are in clumps. Why? Think of the model we fitted (i.e., one with a categorical explanatory variable)] 6.3.2 Normal quantile-quantile (QQ) plot The residuals are normally distributed assumption can be checked with this plot. This plot shows the sorted residuals versus corresponding quantile expected from a standard normal distribution. Points should be close to the dashed diagonal line; points moving away from the line at either tail suggest the residuals are from a skewed distribution, although some departure from the dashed diagonal line is acceptable and common. 6.3.3 Scale-Location plot This plot can be used to check the linear relationship assumption. We want the line to be roughly horizontal (means the relationship between Y and X variables is linear). It can also be another way to check the homoscedasticity (constant-variance) assumption. For this assumption to be met in this plot, we want to see that the residuals are spread roughly equally along the range of the explanatory variable/s (fitted values). 6.3.4 Residuals vs Leverage plot This plot can also be used to check the linear relationship and residuals having constant variance assumptions in the same way as the scale-location plot: we want to see a roughly straight horizontal line and the spread of standardized residuals shouldn’t change as a function of leverage. The other useful functionality of this plot is it can help detect influential outliers in a linear regression model (i.e. Even if data have extreme values, they might not be influential to determine the regression line.). Points with high leverage may be influential: that is, deleting them would change the model a lot. These high leverage points will be found in upper or lower right hand corners of the plot. [Note: in the residuals vs leverage plot above things look a bit weird because there are only three points. Why? Well think of the model we fitted (i.e., one with a single categorical explanatory variable). It looks like there is a plot up in the top right-hand corner, but actually the three points are all centered on 0 on the y-axis. We can’t tell much from this plot, but will use these plots more later on.] 6.3.5 What if data do not fulfil the model assumptions? We can test these model assumptions more formally using specific tests, but in general, these visual model diagnostic plots are enough to understand our data. A pattern in these diagnostic plots is not just a go-or-stop sign. It makes us think about our data and ask some questions: Are there some points in your data set that are identified as potentially problematic in more than one diagnostic plot? Go back to your original data set - is there something special about that point? Or is it an error in data entry? Is there really a linear relationship between Y and X variables? If not, you can transform your data to attempt to make it linear (makes interpretation of results a bit more complicated) or use a different statistical technique (some sort of non-linear regression) (not covered in this course) Is there any other important variable/s that you left out of your model? Could data be systematically biased from data collection? Checking residuals is a great way to discover new insights in your model and data! More information here on understanding diagnostic plots for linear regression analysis Important things to note: run model diagnostics AFTER you’ve fitted the linear model to the data (using lm()) but BEFORE you interpret the outputs if model assumptions are not met/violated, then the results of our hypothesis test may be inaccurate and so we should not interpret the outputs. 6.4 Other resources: optional but recommended I thought it could be helpful to have a thread on ANOVA in R. As a statistical consultant, this is the most frequent FAQ I get from clients - how to run a linear model on their data, conduct hypothesis tests, extract predicted means and perform contrasts. — We are R-Ladies (@WeAreRLadies) February 2, 2020 I've made this cheat sheet and I think it's important. Most stats 101 tests are simple linear models - including \"non-parametric\" tests. It's so simple we should only teach regression. Avoid confusing students with a zoo of named tests. https://t.co/9PFR1ly3lW 1/n — Jonas K. Lindeløv (@jonaslindeloev) March 27, 2019 The aov() function in #Rstats is actually a wrapper around the lm() function pic.twitter.com/FbvxQdtD4c — Dan Quintana (@dsquintana) October 30, 2019 How to calculate the Standard Error of the Mean How do we reflect our uncertainty about our sample mean? (remember it’s the population we want to make inference on based on our sample!) The Standard Error of the Mean, SEM, \\(= \\frac{\\sigma}{\\sqrt{n}}\\); where \\(\\sigma\\) is a population parameter that we estimate using our sample standard deviation (SD). \\(SD = \\sqrt{\\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) (\\(i = 1,...,n\\)) is the standard deviation (SD) of the sample, \\(n\\) is the sample size, and \\(\\bar{x}\\) is the sample mean. Calculating the top line of the SD calculation \\(\\sum_{i = 1}^n(x_i - \\bar{x})^2, i = 1,...,n\\) by hand. It’s the sum squared differences of the distances between the \\(i^{th}\\) observation and the sample mean \\(\\bar{x}\\) (denoted \\(\\mu_x\\) in the GIF below) So using the example values in the GIF # our sample of values x &lt;- c(1,2,3,5,6,9) # sample mean sample_mean &lt;- mean(x) sample_mean ## [1] 4.333333 # distance from mean for each value distance_from_mean &lt;- x - sample_mean distance_from_mean ## [1] -3.3333333 -2.3333333 -1.3333333 0.6666667 1.6666667 4.6666667 # squared distance from mean for each value squared_distance_from_mean &lt;- distance_from_mean^2 squared_distance_from_mean ## [1] 11.1111111 5.4444444 1.7777778 0.4444444 2.7777778 21.7777778 # sum of the squared distances sum(squared_distance_from_mean) ## [1] 43.33333 Calculating SD and SEM So that’s the top line, what about the complete SD calculation? Remember it’s \\(\\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) so = \\(\\sqrt{\\frac{43.3333333}{n-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{6-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{5}}\\) = 2.9439203. Or we could just use R’s sd() function (much easier!!) sd(x) ## [1] 2.94392 So the SEM is \\(\\frac{\\text{SD}}{\\sqrt{n}}\\) = \\(\\frac{2.9439203}{\\sqrt{6}}\\) = 1.20185 In R # (length(x)) returns the length of the vector x which is n, the sample size sd(x)/sqrt(length(x)) ## [1] 1.20185 For the paua data we can avoid all those “by-hand” calculations and use the in-built functions in R to calculate the mean and the SEM, which we need for our test statistic, the t-statistic. sem &lt;- paua %&gt;% summarise(mean = mean(Length), sem = sd(Length)/sqrt(length(Length))) sem ## # A tibble: 1 × 2 ## mean sem ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.19 0.155 "],["statistical-inference.html", "Chapter 7 Statistical Inference 7.1 Linear Regression 7.2 Model comparison, selection, and checking (again) 7.3 Confidence intervals and point predictions 7.4 TL;DR lm() 7.5 Other resources: optional but recommended 7.6 Beyond Linear Models to Generalised Linear Models (GLMs) (not examinable)", " Chapter 7 Statistical Inference Learning outcomes Write R code to fit a linear regression model, and interpret estimated coefficients of these models: no explanatory variable (a null model) a single continuous explanatory variable a single categorical explanatory variable (using dummy variables) a continuous and a categorical factor explanatory variable an interaction term in the explanatory variables Explain why you may want to include interaction terms in a model Describe the differences between the operators : and * in an R model-fitting formula Demonstrate how to make point predictions from linear regression models, specifically models with: a single continuous explanatory variable a continuous and a categorical factor explanatory variable an interaction term in the explanatory variables Understand how to undertake model selection using the anova() and AIC() functions in R and interpret the output Compute confidence intervals in R to estimate a population parameter, and interpret their meaning 7.1 Linear Regression 7.1.1 Some mathematical notation Let’s consider a linear regression with a single explanatory variable. This is what the model looks like: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated) Note: you may also see the intercept term \\(\\alpha\\) written as \\(\\beta_0\\) Does this remind you of anything? 7.1.2 Modelling Bill Depth In the following statistical inference sections, we will be working with our palmerpenguins data set, where we are interested in examining a few different linear regression models with a different numbers of explanatory variables that may help us explain the variation in bill depth bill_depth_mm, our response variable. We will start with the simplest model, with no explanatory variables, and then add in more explanatory variables (both categorical and continuous) and alter the way they interact with each other in the model, while seeing how to use this sample data to draw inferences about the underlying population. Let’s take a look at the bill depth data in a histogram (it’s always nice to have a visual!). library(tidyverse) library(palmerpenguins) penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(data = penguins_nafree, aes(x = bill_depth_mm)) + geom_histogram() + theme_classic() + xlab(&quot;Bill depth (mm)&quot;) 7.1.3 Intercept only (null) model First off let’s fit a null (intercept only, no slope) model. This in old money would be called a one sample t-test. Model formula The null model formula is: \\[Y_i = \\alpha + \\epsilon_i.\\] Here for observation \\(i\\), \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(\\alpha\\) is a parameter to be estimated (typically called the intercept). The R code for this model specifies a 1 to represent a constant, as we have no explanatory variables in this model and there is no slope to estimate. slm_null &lt;- lm(bill_depth_mm ~ 1, data = penguins_nafree) The hypotheses being tested are \\(H_0:\\mu_{\\text{bill_depth_mm}} = 0\\) vs. \\(H_1:\\mu_{\\text{bill_depth_mm}} \\neq 0\\) summary(slm_null)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.16486 0.1079134 159.0614 1.965076e-315 So, what are the values/regression coefficients from the lm() output above? Estimate of the (Intercept) = 17.1648649, tells us the (estimated) average value of the response (in this example, it is the estimate of the population mean (\\(\\mu\\)) of bill_depth_mm), which we can show using the code below: penguins_nafree %&gt;% summarise(average_bill_depth = mean(bill_depth_mm)) ## # A tibble: 1 × 1 ## average_bill_depth ## &lt;dbl&gt; ## 1 17.2 The SEM (Std. Error) = 0.1079134. The t-statistic is given by t value = Estimate / Std. Error = 159.0614207 The p-value is given byPr (&gt;|t|) = 1.965076e-315 So the probability of observing a t-statistic as least as extreme given under the null hypothesis (average bill depth = 0) given our data is 1.965076e-315, pretty strong evidence against the null hypothesis I’d say! I hope this makes sense biologically as we would not expect the average bill depth of a penguin to be 0! 7.1.4 Single continuous explanatory variable Does bill_length_mm help explain some of the variation in bill_depth_mm? Model formula The formula for this model is \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(x_i\\) is the value of the explanatory variable (bill_length_mm); \\(\\alpha\\) and \\(\\beta_1\\) are population parameters to be estimated using our sample data. We could also write this model as \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\epsilon \\end{aligned} \\] Fitted model We can output our estimated population parameters (\\(\\alpha\\) and \\(\\beta_1\\)) using slm &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins_nafree) summary(slm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.78664867 0.85417308 24.335406 1.026904e-75 ## bill_length_mm -0.08232675 0.01926835 -4.272642 2.528290e-05 The Estimate of the (Intercept) (\\(\\alpha\\) in the model) gives us the estimated average bill depth (mm) given the estimated relationship between bill length (mm) and bill depth (mm). The Estimate of bill_length_mm (\\(\\beta_1\\) in the model) is the slope associated with bill length (mm). Here for every 1mm increase in bill length we estimated a 0.082 mm decrease (or a -0.082 mm increase) in bill depth. The plot below shows this relationship visually: ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm) ## plot ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 7.1.5 A continuous and a categorical factor explanatory variable (Additive model) Does adding in species to the above model help explain more of the variation in bill_depth_mm? Remember species is a categorical factor variable with three levels/groups: Adelie, Chinstrap and Gentoo. p2 &lt;- ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p2 # notice the + between the two explanatory variables? # this + is what indicates this is an additive model slm_sp &lt;- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins_nafree) Model formula Now we have two explanatory variables, if they were both continuous variables, the model formula would be… \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2)\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) \\(z_i\\) is one explanatory variable (bill_length_mm say) \\(x_i\\) is another explanatory variable (species say) \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\) are all parameters to be estimated. …but because one of the variables is categorical, our model formula needs to be written in a slightly more complicated way \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_{i,1} + \\beta_3x_{i,2} + ... +\\beta_kx_{i,k}+ \\epsilon_i\\] where the only new terminology from the above formula is \\(x_{i,k}\\) is the value of the explanatory variable (where k = # of levels of the variable - 1, as one level is the intercept) Another way to write this model formula for this example is \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] When we have categorical explanatory variables (e.g., species) we have to use dummy variables to estimate population parameters in our linear regression model. What is a dummy variable? It is a variable created to assign numerical value to levels of categorical variables. This is necessary with most categorical variables because an individual usually cannot be in more than one group/level (e.g., a Chinstrap penguin cannot be a Gentoo penguin too!). So the dummy variable coding works by representing one category of the explanatory variable and is coded with 1 if the case falls in that category and with 0 if not. Here’s an example dummy variable table for species in the palmerpenguin data set: Here the Adelie group is the baseline (R does this alphabetically by default. To change this, we can use the mutate function. See section 5.2.4.2 in Hypothesis testing for more details) so we assign a 0 to speciesChinstrap and a 0 to speciesGentoo to see what the response is for Adelie penguins (Intercept). This is shown in the top line row of the table above. Fitted model summary(slm_sp)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5652616 0.69092642 15.291442 2.977289e-40 ## bill_length_mm 0.2004431 0.01767974 11.337449 2.258955e-25 ## speciesChinstrap -1.9330779 0.22571878 -8.564099 4.259893e-16 ## speciesGentoo -5.1033153 0.19439523 -26.252267 1.043789e-82 Here, the Estimate of the (Intercept) (\\(\\alpha\\) or \\(\\beta_0\\) in the model) gives us the estimated average bill depth (mm) of the Adelie penguins given the other variables in the model. The Estimate of bill_length_mm (\\(\\beta_1\\) in the model) is the slope associated with bill length (mm). So, here for every 1 mm increase in bill length we estimated a 0.2 mm increase in bill depth. What about the regression coefficients of the other species levels (Estimate for speciesChinstrap and speciesGentoo)? These values give the shift (up or down) of the parallel lines from the Adelie ((Intercept)) level. So given the estimated relationship between bill depth and bill length these coefficients are the estimated change for each species from the baseline. You can visualise this by looking at the plot below. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_sp) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 7.1.6 A continuous and a categorical factor explanatory variable (Interaction model) Recall the additive model formula from above \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] An additive model will always produce regression lines that are parallel with each other (i.e., there is no relationship between the explanatory variables, they act independently of each other)… …but we should really check to see if the explanatory variables have relationships with each other (i.e., the explanatory variables do not act independently on the response variable.). If this is the case, the regression lines will not be parallel with each other. Model formula This is still for two explanatory variables \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_{i,1} + \\beta_3x_{i,2} + ... +\\beta_kx_{i,k}+ \\beta_4z_ix_{i,k} + \\epsilon_i\\] You’ll notice a new term \\(\\beta_4z_ix_{i,k}\\) in this model compared to the additive model. This is the interaction term between the two explanatory variables \\(z_i\\) and \\(x_{i,k}\\). The model formula for our penguin example can also be written as \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{5}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) + \\epsilon \\end{aligned} \\] where \\(\\beta_4\\)bill_length_mm x speciesChinstrap and \\(\\beta_5\\)bill_length_mm x speciesGentoo represent the interaction terms in the model. This is getting pretty complicated already and we only have two explanatory variables!!! However, we can add in more explanatory variables, both categorical and/or continuous, with additive and/or interaction terms between variables and the same principles for inference apply. We will practice with slightly more complicated data in our lab assignment this week. Note: we can include interaction effects in our lm() model in R by using either the * or : syntax in our model formula. For example, : denotes the interaction only of the variables to its left and right, and * means to include all main effects and interactions, so a * b is the same as a + b + a:b, where a and b represent the main effects and a:b represents the interaction term in the model. To specify a model with additive and interaction effects use either slm_int &lt;- lm(bill_depth_mm ~ bill_length_mm * species, data = penguins_nafree) Or the other way we could write this and get the SAME MODEL is slm_int &lt;- lm(bill_depth_mm ~ bill_length_mm + species + bill_length_mm : species, data = penguins_nafree) Fitted model summary(slm_int)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.48770713 1.15987305 9.9042797 2.135979e-20 ## bill_length_mm 0.17668344 0.02980564 5.9278518 7.793199e-09 ## speciesChinstrap -3.91856701 2.06730876 -1.8954919 5.890889e-02 ## speciesGentoo -6.36675118 1.77989710 -3.5770333 4.000274e-04 ## bill_length_mm:speciesChinstrap 0.04552828 0.04594283 0.9909769 3.224296e-01 ## bill_length_mm:speciesGentoo 0.03092816 0.04111608 0.7522157 4.524625e-01 As before, the Estimate of the (Intercept) gives us the estimated average bill depth (mm) of the Adelie penguins given the other variables in the model. The bill_length_mm : Estimate (\\(\\beta_1\\) in the model) is the slope associated with bill length (mm). So, here for every 1 mm increase in bill length we estimated a 0.177 mm increase in bill depth. The main effects of species (i.e., Estimate for speciesChinstrap and speciesGentoo) again give the shift (up or down) of the lines from the Adelie level; however these lines are no longer parallel! The interaction terms (i.e., bill_length_mm:speciesChinstrap and bill_length_mm:speciesGentoo) specify the species specific slopes given the other variables in the model. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_int) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) Look at the plot above. Now we’ve specified this all singing and dancing interaction model we might ask are the non-parallel lines non-parallel enough to say this model is better than the parallel line model? 7.2 Model comparison, selection, and checking (again) Remember that it is always is imperative that we check the underlying assumptions of our model! If our assumptions are not met then basically the maths falls over and we can’t reliably interpret the outputs from the model (e.g., can’t trust the parameter estimates etc.). Key assumptions of linear regression models Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed Let’s look at the fit of the slm model (single continuous explanatory variable) gglm::gglm(slm) # Plot the four main diagnostic plots Do you think the model assumptions have been met? Think of what this model is, do you think it’s the best we can do? 7.2.1 Model comparison and selection A good model not only needs to fit data well, it also needs to be parsimonious. That is, a good model should be only be as complex as necessary to describe a data set. If you are choosing between a very simple model with 1 explanatory variable, and a very complex model with, say, 10 explanatory variables, the very complex model needs to provide a much better fit to the data in order to justify its increased complexity. If it can’t, then the simpler model should be preferred. After all, it’s also much easier to interpret a simpler model (look at our increased complexity from just adding the interaction term in a two explanatory variable model)! 7.2.1.1 Model comparison using anova() We can compare nested linear models using hypothesis testing. Luckily the R function anova() automates this if you input the two regression objects as separate arguments. Yes, it’s the same idea as we’ve previously learnt about ANOVA. We perform an F-test between the nested models! By nested we mean that one model is a subset of the other (i.e., where the coefficients both models have in common cancel each other out, so they are essentially weighted at zero). For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\epsilon_i\\] is a nested version of \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] This ANOVA will test whether or not including \\(\\beta_2x_i\\) leads to a significant improvement in the model. As an example consider testing the single explanatory variable model slm against the same model with species included as a variable slm_sp. To carry out the appropriate hypothesis test in R we can run anova(slm,slm_sp) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm ## Model 2: bill_depth_mm ~ bill_length_mm + species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 331 1220.16 ## 2 329 299.62 2 920.55 505.41 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As you’ll see the anova() function takes two model objects (slm and slm_sp) each as arguments. It returns an ANOVA testing whether the simpler model (slm) is just as good at capturing the variation in the data as the more complex model (slm_sp). The returned p-value should be interpreted as in any other hypothesis test. i.e., the probability of observing a statistic as least as extreme under our null hypothesis (here, that either model is as good as each other at capturing the variation in the data, or model 1 = model 2, or model 1 - model 2 = 0). What would we conclude here from the output above comparing slm and slm_sp? We can see the results show a Df of 2 (indicating that the more complex model has two additional parameters) and a very small p-value (&lt;2e-16). I’d say we have very strong evidence against the models being equally good, and adding species to the model lead to a significantly improved fit over slm without species! Looking at the relevant plots, does this make sense? Now what about slm_int vs slm_sp? Are the non-parallel lines non-parallel enough to say the more complicated interaction model is better than the simpler parallel line additive model? anova(slm_sp,slm_int) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm + species ## Model 2: bill_depth_mm ~ bill_length_mm + species + bill_length_mm:species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 329 299.62 ## 2 327 298.62 2 0.99284 0.5436 0.5812 So it seems both models are just as good at capturing the variation in our data: we’re happy with the simpler model with parallel lines! 7.2.1.2 Model comparison using AIC() Another way we might compare models is by using the Akaike information criterion (AIC) (you’ll see more of this later in the course). AIC is an estimator of out-of-sample prediction error and can be used as a metric to choose between competing models. Lower AIC scores are better, and AIC penalizes models that use more parameters. So if two models explain the same amount of variation, the one with fewer parameters (df) will have a lower AIC value (i.e., smallest out-of-sample prediction error) and will be the better-fit model. Typically, a difference of 4 or more is considered to indicate an improvement; this should not be taken as writ however, using multiple comparison techniques is advised. R has an AIC() function that can be used directly on your lm() model objects. The AIC() function can take multiple arguments compared to anova() that can only compare two models at a time. For example, AIC(slm_null, slm, slm_sp, slm_int) ## df AIC ## slm_null 2 1399.3234 ## slm 3 1383.4462 ## slm_sp 5 919.8347 ## slm_int 7 922.7294 This supports what the anova() output suggested; slm_sp is the preferred model. As always it’s important to do a sanity check! Does this make sense? Have a look at the outputs from these models and see what you think. Just because we’ve chosen a model (the best of a bad bunch perhaps?) this doesn’t let us off the hook. We should check our assumptions… …well, we should have checked them before we started to interpret the data! gglm::gglm(slm_sp) # Plot the four main diagnostic plots Residuals vs Fitted plot: equal spread? Doesn’t look too trumpety/slice of pizza shaped! Don’t worry that there aren’t many points between 16-17 on the x-axis. Normal quantile-quantile (QQ) plot: skewed? Maybe slightly right skewed (deviation upwards from the right tail) but probably nothing to worry about. Scale-Location plot: equal spread and relatively straight line? I’d say so. Residuals vs Leverage: ? Maybe a couple of points with high leverage. 7.3 Confidence intervals and point predictions So far, we’ve got population parameter estimates from our linear regression models (all the \\(\\alpha\\)s and \\(\\beta\\)s!). We’ve used other statistical tools to check the linear regression model assumptions are met and also selected the best model (in terms of which explanatory variables are most useful, and whether they should be treated as independent from each other (additive) or interacting). However, there’s a couple more things we might like to do, such as: determine confidence intervals for parameter estimates make point predictions (i.e., input explanatory variable information to predict the response variable) 7.3.1 Confidence intervals Confidence intervals provide a range of values around a population estimate that are believed to contain, with a certain probability (e.g., 95%), the true value of that estimate. They provide a way to quantify the uncertainty about the value of that population estimate we have calculated from the sample data. For the chosen slm_sp model we can get the confidence intervals for the regression coefficient estimates in R by using the confint() function. cis &lt;- confint(slm_sp) cis ## 2.5 % 97.5 % ## (Intercept) 9.2060707 11.9244526 ## bill_length_mm 0.1656635 0.2352227 ## speciesChinstrap -2.3771120 -1.4890438 ## speciesGentoo -5.4857298 -4.7209009 # By default the 95% confidence intervals are returned # you can change this using the `level = ` argument This tells us that for every 1 mm increase in bill length we are 95% confident the expected bill depth will increase between 0.166 and 0.235 mm (3 sf). We are 95% confident that the average bill depth of a Chinstrap penguin is between 1.5 and 2.4 mm shallower than the Adelie penguin. To interpret these correctly we must understand what they are telling us. That is, 95% of the time, our 95% confidence intervals taken from a random sample will contain the true population parameter we are wanting to estimate. Check out this cool visualisation of CI’s 7.3.2 Point prediction Using the slm_sp model we can make a point prediction for the expected bill depth (mm) for Gentoo penguins with a bill length of 50mm. Recall the model equation \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] And recall the output table from this model, with estimates. summary(slm_sp)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5652616 0.69092642 15.291442 2.977289e-40 ## bill_length_mm 0.2004431 0.01767974 11.337449 2.258955e-25 ## speciesChinstrap -1.9330779 0.22571878 -8.564099 4.259893e-16 ## speciesGentoo -5.1033153 0.19439523 -26.252267 1.043789e-82 We can then substitute in the values (using our dummy variable coding for species): \\[ \\begin{aligned} \\widehat{\\text{bill depth}} &amp; = \\hat{\\alpha} + \\hat{\\beta_1}*50 + \\hat{\\beta_2}*0 + \\hat{\\beta_3}*1\\\\ \\ &amp; = 10.56 + 0.20*50 - 1.93*0- 5.10*1\\\\ \\ &amp; = 10.56 + 10 - 5.10\\\\ \\ &amp; = 15.46\\text{mm} \\end{aligned} \\] Rather than by hand we can do this easily in R ## create new data frame with data we want to predict ## the names have to match those in our original data frame newdf &lt;- data.frame(species = &quot;Gentoo&quot;, bill_length_mm = 50) ## use predict() function predict(slm_sp, newdata = newdf) ## more accurate than our by hand version! ## 1 ## 15.4841 What does this look like on a plot? 7.4 TL;DR lm() Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) Artwork by @allison_horst Meet your MLR teaching assistants Interpret coefficients for categorical predictor variables Interpret coefficients for continuous predictor variables Make predictions using the regression model Residuals Check residuals for normality 7.4.1 Model formula syntax In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicates to include the variable to the left and right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additionally, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a * b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., \\(x^2\\)). Here the expression y ~ x^2 does not relate to squaring the explanatory variable \\(x\\) (this is to do with the syntax ^ you see above. To include \\(x^2\\) as a term in our model we have to use the I() (the “as-is” operator). For example, y ~ I(x^2)). 7.5 Other resources: optional but recommended Exploring interactions with continuous predictors in regression models The ASA Statement on p-Values: Context, Process, and Purpose 7.6 Beyond Linear Models to Generalised Linear Models (GLMs) (not examinable) Recall the assumptions of a linear model The \\(i\\)th observation’s response, \\(Y_i\\), comes from a normal distribution Its mean, \\(\\mu_i\\), is a linear combination of the explanatory terms Its variance, \\(\\sigma^2\\), is the same for all observations Each observation’s response is independent of all others But, what if we want to rid ourselves from a model with normal errors? The answer: Generalised Linear Models. 7.6.1 Counting animals… A normal distribution does not adequately describe the response, the number of animals It is a continuous distribution, but the response is discrete It is symmetric, but the response is unlikely to be so It is unbounded, and assumes it is plausible for the response to be negative In addition, a linear regression model typically assumes constant variance, but in this situation this unlikely to be the case. So why assume a normal distribution? Let’s use a Poisson distribution instead. \\[\\begin{equation*} \\mu_i = \\beta_0 + \\beta_1 x_i, \\end{equation*}\\] So \\[\\begin{equation*} Y_i \\sim \\text{Normal}(\\mu_i\\, \\sigma^2), \\end{equation*}\\] becomes \\[\\begin{equation*} Y_i \\sim \\text{Poisson}(\\mu_i), \\end{equation*}\\] The Poisson distribution is commonly used as a general-purpose distribution for counts. A key feature of this distribution is \\(\\text{Var}(Y_i) = \\mu_i\\), so we expect the variance to increase with the mean. 7.6.2 Other modelling approaches (not examinable) R function Use glm() Fit a linear model with a specific error structure specified using the family = argument (Poisson, binomial, gamma) gam() Fit a generalised additive model. The R package mgcv must be loaded lme() and nlme() Fit linear and non-linear mixed effects models. The R package nlme must be loaded lmer() Fit linear and generalised linear and non-linear mixed effects models. The package lme4 must be installed and loaded gls() Fit generalised least squares models. The R package nlme must be loaded "],["multivariate-data-analysis.html", "Chapter 8 Multivariate Data Analysis 8.1 What is multivariate data? 8.2 Dimension reduction 8.3 Distance matrices and measures 8.4 Why is nMDS used frequently in biology? 8.5 Steps to nMDS 8.6 nMDS in R using metaMDS() on the palmerpenguins data set 8.7 Diagnostic plots for nMDS 8.8 How to interpret an nMDS plot and what to report 8.9 nMDS in R using metaMDS() with community structure data 8.10 Other resources: optional but recommended", " Chapter 8 Multivariate Data Analysis Learning outcomes Define multivariate data Describe what dimension reduction is and why we apply it Explain the aims, steps and uses of non-metric multidimensional scaling (nMDS) for biological application Compare two different distance measures, namely Euclidean and Bray Curtis, and know when it is appropriate to use each one Write R code to carry out nMDS, plot a 2-D solution and perform associated diagnostic plots Interpret and effectively communicate nMDS outputs 8.1 What is multivariate data? So far in this course we have visualised and analysed data with, at most a few variables, where each variable generally requires a dimension in space or a separate axis on a graph to be visualised (e.g., if we have 8 variables in a data set, we would require 8 dimensions/an 8-axis graph to show them all). We are pretty good at looking for patterns in data in 1- and 2-dimensions (2-D), we’re ok with 3-D, but for most people the thought of visualising what 4-D or above even looks like is mind-boggling enough without even thinking about interpreting data in these high dimensions!! Unfortunately, most biological data sets contain many variables (aka is high-dimensional or multivariate), so we can do one of two things: Analyse the data in pairs of variables, which can kind of work for smallish multivariate data sets however, this quickly gets challenging to make overall interpretations of the data set as a whole. Use statistical tools to search for patterns in the data in multivariate space and summarise those patterns into a lower dimensionality so our brains can process it!! Let’s elaborate more on the second option presented here… 8.2 Dimension reduction When data is high-dimensional, it is hard to distinguish between the important variables that are relevant to the output and the redundant or not-so important ones. Dimension reduction can make the data less noisy while retaining the important information and simplify it for exploration, interpretation, and visualisation. The main aim of dimension reduction is to find the best low-dimensional representation of the variation in a multivariate data set, but how do we do this? There are many methods that all have slightly different uses, including principal component analysis (PCA), principal coordinate analysis (PCO), cluster analysis, discriminant analysis etc. – and one that is commonly used in many areas of biology that we are going to explore in more detail is non-metric multidimensional scaling (nMDS). 8.2.1 Non-metric multidimensional scaling (nMDS) The aim of nMDS is to ‘map’ the original position of samples in a dataset in multidimensional space as accurately as possible using a reduced number of dimensions. This means the distances between the sample points in the specified reduced dimensions (usually 2-D because this is easiest for our brains to visualise, but sometimes 3-D if required) is as closely matched as possible to the relative distances among the samples from the original higher dimension dataset. For example, if sample A has higher similarity to sample B than it does to sample C then sample A will be placed closer on the map to sample B than it is to sample C. This makes it easier for the original data to be plotted, visualized and interpreted. 8.3 Distance matrices and measures To create a nMDS visualisation, the original data must first be translated into a distance matrix, as shown in the diagram below. R will automatically calculate the distance matrix for us using the original data when we write code to produce an nMDS plot, but it’s important we know what a distance matrix is and how it is calculated. The reason is we need to input the type of distance matrix we want R to use, which differs depending on your data. An example of a distance matrix you may have seen before is distances between cities on a map. Below is a distance matrix for flight distances (in kilometres) between locations in the North Island of New Zealand. Since distance is symmetric (i.e. distance between A and B is equal to distance between B and A), we can focus on only the lower (orange) or upper (green) triangular part of the matrix. The diagonal elements of a distance matrix (blue) are zero because they represent the distance of a sample from itself. So this information is often shown in a triangular matrix, as below. Another way we could measure the distance between these cities would be to use driving distances. These distances differ from flight distances as the roads are not usually in a straight line to a location; instead they go around mountains, through valleys and connect towns and cities together. Below is a distance matrix for driving distances (in kilometres) between locations in the North Island of New Zealand. You should note that all the flight distances are shorter than the driving distances. Link to source We could also display distance matrices of flight times, driving times etc. There are many ways to calculate distance between these geographic locations! In a similar way, there are also many distance measures we can use to determine differences between biological samples See Legendre &amp; Legendre’s 1998 (&amp; 2012) book Numerical Ecology. We will introduce two commonly used distance measures in biology – Euclidean distance and Bray-Curtis distance. 8.3.1 Euclidean distance Drawing a straight line between A and B is also termed “as the crow flies” distance. This is how the flight distances in the example above were measured and is also called Euclidean distance - a very commonly used distance measure. It is the way we measure most things in life! Above we have two points A and B (these could be biological samples or observations) and we want to know the straight line, as the crow flies, distance between them so we can use the Euclidean distance formula to calculate this. (Note: if you remember a bit of high school mathematics, you may notice this is Pythagoras’ formula). Let’s say x1 = 1, x2 = 5, y1 = 1 and y2 = 3. We could write this as A (1, 1) and B (5, 3). \\[ \\begin{aligned} d_{Euclidean}(A, B) &amp; = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\\\ &amp; = \\sqrt{(5 - 1)^2 + (3 - 1)^2}\\\\ &amp; = \\sqrt{(4)^2 + (2)^2}\\\\ &amp; = \\sqrt{16 + 4}\\\\ &amp; = \\sqrt{20}\\\\ &amp; = 4.47 \\end{aligned} \\] This example only has two variables (x and y) so only two dimensions, however, we can use this formula to calculate distance between two points (or samples with biological data) for any number of variables in multivariate space (or R can do it for us!). A more generic way to write the formula for multivariate data is: \\[d_{Euclidean}(j, k) = \\sqrt{\\sum^{n} (y_{i,j} - y_{i,k})^2}\\] where j and k represent the two samples whose distance is being measured, so \\(y_{i,j}\\) is the value of variable i in sample j, and \\(y_{i,k}\\) is the value of variable i in sample k, and n is the total number of samples. “the value of the variable i in sample j” would be equivalent to the abundance of a particular species i in a location/sample j. Below is a made-up ecological data set with four samples (S1 - S4, let’s say they are different locations) and seven variables (V1 - V7, let’s say they are different species). ## Variable ## Sample V1 V2 V3 V4 V5 V6 V7 ## S1 0 5 3 0 2 0 0 ## S2 0 10 6 0 4 0 0 ## S3 1 0 0 1 0 1 0 ## S4 0 1 1 0 1 0 0 Take a minute to study this data. which samples/locations do you think are most similar to each other? which samples/locations are most different from each other? R will calculate the distance matrix for us, but let’s practice calculating the Euclidean distance between sample 1 &amp; 2. (doing these calculations is not examinable) \\[ \\begin{aligned} d_{Euclidean}(j, k) &amp; = \\sqrt{\\sum^{n} (y_{i,j} - y_{i,k})^2}\\\\ d_{Euclidean}(1, 2) &amp; = \\sqrt{\\sum(y_{(V1,S1)} - y_{(V1,S2})^2 + (y_{(V2,S1)} - y_{(V2,S2)})^2 + ... +(y_{(V7,S1)} - y_{(V7,S2)})^2}\\\\ &amp; = \\sqrt{(0 - 0)^2 + (5 - 10)^2 + (3 - 6)^2 + (0 - 0)^2 + (2 - 4)^2 + (0 - 0)^2 + (0 - 0)^2}\\\\ &amp; = \\sqrt{(-5)^2 + (-3)^2 + (-2)^2}\\\\ &amp; = \\sqrt{38}\\\\ &amp; = 6.1644 \\end{aligned} \\] This means the distance between samples 1 and 2 in 7-D space is 6.1644. The complete triangular distance matrix using the Euclidean distance measure is: ## 1 2 3 4 ## 1 ## 2 6.1644 ## 3 6.4031 12.4500 ## 4 4.5826 10.7240 2.4495 Now take a minute to study the values in the distance matrix above. Larger values mean they are further apart/less similar to each other. Is this the same as what you expected to see when looking at the original data set? Samples 2 &amp; 4 appear very different from each other with a distance of 10.724, but they share all the same species. Samples 3 &amp; 4 appear the most similar to each other with a distance of 2.4495, even though they have no species in common. What’s going on here? Euclidean distance… …is a really useful distance measure for “well-behaved” data (without many zeroes or the zeroes are meaningful) and approximately normally distributed data!! …does not have an upper value boundary (can be any number larger than zero). …puts emphasis on variables with large ranges (high abundance values) e.g., samples 2 &amp; 4. …between two samples not sharing any variable/species may be smaller than between two samples sharing all variables/species, but with the same variables/species having large abundance differences between samples e.g., samples 3 &amp; 4 vs samples 2 &amp; 4, respectively. In other words, the distance between 0 and 1 is treated the same as the distance between 1 and 2, or 1001 and 1002. …will put samples with low counts closer together, even when they have no variables/species in common i.e., samples 3 &amp; 4. …is a symmetrical index, i.e., it treats double zeros in the same way as double presences. This means it is not appropriate for data sets where joint absences are considered meaningless, i.e., if two samples do not have a particular variable/species present, does this mean they are more similar to each other? 8.3.2 Bray-Curtis (BC) distance The Bray-Curtis dissimilarity measure is commonly used for many types of biological data (e.g., community composition data, metagenomics, medicine). It takes the sum of the absolute differences between samples (that’s what the |…| symbols mean) and divides by the total sum for a pair of samples. By doing this, it calculates the proportion of mis-matching individuals between samples. The formula is: \\[d_{Bray-Curtis}(j,k) = \\sum^n \\frac{|y_{i,k} - y_{j,k}|}{(y_{i,k} + y_{j,k})}\\] where j and k represent the two samples whose distance is being measured, so \\(y_{i,j}\\) is the value of variable i in sample j, and \\(y_{i,k}\\) is the value of variable i in sample k, and n is the total number of samples. “the value of the variable i in sample j” would be equivalent to the abundance of a particular species i in a location/sample j for community structure data. The result will always take a value between 0 and 1, where 0 = the samples are identical to each other and 1 = the samples have no variables in common. (Note: this is often converted to a Bray-Curtis similarity distance by BCsim = 1 - BCdissim. Why? Because it is more intuitive to interpret 0 = nothing in common and 1 = identical! You need to be aware of whether the dissimilarity or the similarity distance measure is being used, as it can drastically impact your interpretation!!) Let’s use the same made-up data set we used before to calculate a Bray-Curtis dissimilarity distance matrix: ## Variable ## Sample V1 V2 V3 V4 V5 V6 V7 ## S1 0 5 3 0 2 0 0 ## S2 0 10 6 0 4 0 0 ## S3 1 0 0 1 0 1 0 ## S4 0 1 1 0 1 0 0 As mentioned earlier, R will calculate the distance matrix for us, but let’s practice calculating the Bray-Curtis distance between sample 1 &amp; 2. (doing these calculations is not examinable) \\[ \\begin{aligned} d_{Bray-Curtis}(j,k)&amp; = \\sum^n \\frac{|y_{i,k} - y_{j,k}|}{(y_{i,k} + y_{j,k})}\\\\ d_{Bray-Curtis}(1,2)&amp; = \\frac{|(y_{(V1,S1)} - y_{(V1,S2}) + (y_{(V2,S1)} - y_{(V2,S2)}) + ... +(y_{(V7,S1)} - y_{(V7,S2)})|} {(y_{(V1,S1)} + y_{(V1,S2}) + (y_{(V2,S1)} + y_{(V2,S2)}) + ... +(y_{(V7,S1)} + y_{(V7,S2)})}\\\\ &amp; = \\frac{|0-0| + |5-10| + |3-6| + |0-0| + |2-4| + |0-0| + |0-0|}{(0+0) + (5+10) + (3+6) + (0+0) + (2+4) + (0+0) + (0+0)}\\\\ &amp; = \\frac{5 + 3 + 2}{15 + 9 + 6}\\\\ &amp; = \\frac{10}{30}\\\\ &amp; = 0.333 \\end{aligned} \\] (out of a total of 30 individuals in samples 1 and 2, 10 cannot be paired with an individual of the same species/variable from the other location/sample) The complete triangular distance matrix using the Bray-Curtis dissimilarity distance measure is: ## 1 2 3 4 ## 1 ## 2 0.333 ## 3 1.000 1.000 ## 4 0.538 0.739 1.000 You will see that this differs significantly from the Euclidean distance matrix. Samples 1 &amp; 2 appear the most similar to each other and sample 3 differs completely from the other three samples. This may be more like what you expected to see when you first looked at this data set. This illustrates why it is important to choose the right distance measure specific to your data set :-) Bray-Curtis dissimilarity distance… …reflects changes in composition and relative abundances pretty well. …adds more weighting to relatively rare individuals, as differences between smaller values matter. …always treats joint absences as unimportant – they do not make the samples more similar to each other. 8.4 Why is nMDS used frequently in biology? It makes few (if any) model assumptions about the form of the data. As the name suggests, it is a non-parametric (“non-metric”) technique. This means that data is not required to fit a normal distribution - an assumption of parametric tests, like linear regression we looked at earlier in the course. Fulfilling the multivariate normality assumptions can be very challenging in many biological data sets. What makes nMDS non-metric is that it is rank-based. This means that instead of using the actual values to calculate distances between samples, it uses ranks. For example, instead of saying that sample A is 5 points away from sample B, and 10 from sample C, you would instead say that: sample A is the “1st” most close sample to B, and sample C is the “2nd” most close. These ranks of the similarities between samples are the only information used by nMDS. NOTE: metric MDS (mMDS) would use the actual distances between samples generated in the distance matrix and if the data fit this model, it is the preferred method although we will not be covering this in this course. If we use the Euclidean and the Bray-Curtis distance matrices calculated above, we can display these as the rank-based information that is used to create an nMDS plot. Euclidean distance matrix (left) and resultant rank-based matrix used to create nMDS (right) Bray-Curtis dissimilarity distance matrix (left) and resultant rank-based matrix used to create nMDS (right) Because nMDS uses rank-ordered data, data transformation is less often necessary. Data may require standardisation still – this is especially true if the variables being analysed are on different scales or with completely different units of measurement being used (e.g., grams and millimeters). (we will cover how to do this when working with the penguin data) Some other dimension reduction techniques only work in Euclidean distance (i.e., principal component analysis). nMDS can be performed using any distance measure/metric thus is an extremely flexible technique that can accommodate a variety of different kinds of data. There are many different distance measures to choose from and a commonly used one for biological data is Bray-Curtis. Because of the point made above, the link between the original data and the final plot is relatively transparent and easy to explain – the rationale of nMDS is the preservation of the relationships between samples in reduced dimensional space. 8.5 Steps to nMDS Choose which distance measure is appropriate for your data and calculate the distance matrix to perform the nMDS on. metaMDS() in R creates the distance matrix for you, but your choice of distance measure is important here. Choose a desired number of dimensions for your nMDS. One way to choose an appropriate number of dimensions is to calculate nMDS values for a range of dimensions. A scree plot/stress plot (stress versus number of dimensions) can be easily created in R, to assist in identifying where adding more dimensions does not substantially lower the stress value (more on this in Diagnostic plots for nMDS). As mentioned above, 2- or 3-dimensions is usually favoured for ease of interpretation. Note: although the diagnostic plots are presented later on in this guide, it would be a good idea to run a scree plot early on in your data analysis to help you decide how many dimensions to use for your nMDS. 8.6 nMDS in R using metaMDS() on the palmerpenguins data set R has two main nMDS functions available, monoMDS(), which is part of the MASS library (automatically installed in R), and metaMDS(), which is part of the vegan package. We will use the metaMDS() function because: it is more automated (so is easier for beginners like us!). by default, it ensures the first axis represents the main source of variation in the data (by using principal component analysis - another dimension reduction technique), which is best for interpreting the nMDS plot we will produce. We need to download and install the vegan package, necessary for running metaMDS(). Call the vegan package to use its functions (you will need to install it first if you haven’t already – do you remember how to do that? HINT: install.packages(“”)): library(vegan) We then need to load the data into R, which has columns of variables and rows for samples. We’ll use a familiar data set to start with, our palmerpenguins, which only has a few variables. NOTE: most of the default arguments in metaMDS() are set for community structure data (this is ecological data where we want to know species occurrence and abundance in samples that represent communities) as this is what the vegan package was designed for. As the palmerpenguin data is not community structure data, we will be making some changes to the defaults (this will be useful to know as many of you are not ecologists, or even if you are, your data may not be about community structure!). # we should be used to loading these packages by now :-) library(tidyverse) library(palmerpenguins) # getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na () We only use the numeric variables for our nMDS, so let’s create a pairs plot to look at the relationships between bill_depth_mm, bill_length_mm, flipper_length_mm, and body_mass_g two variables at a time. library(GGally) # introducing a new package GGally, please install then open penguins_nafree %&gt;% ggpairs(columns = 3:6) # specifies to only use numeric variables found in # columns 3 – 6 of the penguins_nafree data set We can overlay more information on to this plot by colouring each sample point by species penguins_nafree %&gt;% ggpairs(columns = 3:6, aes(colour = species)) Or by sex penguins_nafree %&gt;% ggpairs(columns = 3:6, aes(colour = sex)) With four original variables, we need 4-D to visualise this data accurately as a whole. However, we can see that three of these variables (e.g. flipper_length_mm, body_mass_g, and bill_length_mm) have relatively strong positive correlations with each other. Therefore, it seems feasible to display this data in fewer dimensions without losing the important information in the data. Using nMDS will retain the rank order of similarity between samples and because of the correlation between variables, the ranks will be similar between variables, meaning the nMDS should be able to do a good job at reducing dimensionality of the data. Before we run the nMDS, we need to ensure our data is in the correct format. As mentioned earlier, if the variables are on different scales or have different units of measurement, they are not easily comparable to each other. In the penguin data, we have body_mass_g measured in grams while the other three numeric variables are measure in millimetres. A simple tool is to standardise/scale each variable - in R using the scale() function. If using the default arguments, which we will, it will centre each variable by subtracting its overall column mean from each observation and then dividing each observation by the overall column standard deviation: \\(\\frac{x - \\bar{x}}{sx}\\). This makes all variables comparable to each other (i.e. changes within and between variables are relative). One of the outputs from metaMDS() is variable scores, which can be useful for plotting onto our nMDS. For variable scores to be calculated in metaMDS() (called species scores), all of our data must be positive. In community structure data, you cannot have negative species abundances so this is only necessary if using data that contains negative values, or become negative once standardised. To ensure all of our data is positive, we will need to add a constant to each variable so that all of its values are positive. Below, to do this, we have made a minShift() function, which raises each variable by the absolute value of its minimum. For example, if the minimum is -2, +2 is added to each value, so that the minimum is now zero. Finally, to get the variable scores, set wascores=TRUE in metaMDS() code. # This is the code for the minShift function explained above minShift &lt;- function(x) {x + abs(min(x))} # we are creating a new object call nmds_penguins where we are preparing # our non-community structure data for metaMDS() nmds_penguins &lt;- penguins_nafree %&gt;% # use the NA free data set select(where(is.numeric), -year) %&gt;% # year makes no sense here so we remove it scale() %&gt;% # scale the variables apply(. , 2, minShift) # apply our minShift function to the variables/columns Now our data is in a good format, we can run the nMDS, using the metaMDS() function from the vegan package. Remember, the vegan package is designed for community structure data, so we need to change some of the defaults for this analysis. The default distance measure (bray) is only appropriate for certain data (see notes on distance measures above on when you may wish to use it). In many cases with non-community structure data, the distance parameter should be set to euclidean. Euclidean distance is the distance measure we want to use with this penguin data. The default data transformations that are appropriate for community structure data need to be turned off by setting autotransform=FALSE and noshare=FALSE. We can set the number of dimensions/axes we want the nMDS to output. Generally, we do not want more than 3 dimensions/axes as larger numbers means the interpretation of the data remains challenging for our poor brains (anything above 3-D is very difficult to conceptualise and visualise). The number of axes can be changed using the k argument (e.g. k = 2), where k is the number of dimensions. Note for your future-self, should you use these notes again: If you run into a problem where it can’t find a convergent solution (which basically means it can’t find the same solution at least twice), you can increase the number of random restarts to be more likely of finding the global solution for our specified k (e.g., increasing try = and trymax =). nmds_penguins_results &lt;- metaMDS(comm = nmds_penguins, # Define the data to use distance = &quot;euclidean&quot;, # Specify distance k = 2, # can we reduce to 2D for easy interpretation? try = 100, trymax = 100, # Number of random starts autotransform=FALSE, # turn off default for community data noshare=FALSE, # turn off default for community data wascores=TRUE) # turn off default for community data nMDS is an iterative algorithm, so it completes the same series of steps repeatedly until it finds the best solution. The steps are outlined below Begins by constructing an initial random configuration of the samples in the specified number of dimensions. Distances among samples in this starting configuration are calculated and regressed against the original distance matrix. From this regression, the predicted distances for each pair of samples is calculated. If a perfect fit, all distances among samples would match the rank-order of distances in the original distance matrix. A stress value is calculated that measures the goodness-of-fit of this configuration (Kruskal’s stress, which measures the sum of squared differences between this configuration’s distances and the distances predicted by the regression). The configuration is then improved by moving the position of the samples slightly in the direction in which the stress changes most rapidly. The distances among samples is now recalculated, the regression performed again and stress recalculated. This process is repeated until it reaches convergence by failing to achieve any lower stress values, which indicates that a minimum stress level (perhaps local) has been found. This whole procedure will then be repeated with a new random initial configuration (as many as you have specified using the try = and trymax = arguments, which specify the minimum and maximum number of random starts, respectively). If the same lowest stress is achieved at least twice, convergence is reached and this solution is output as the best representation of this high-dimensional multivariate data in the specified number of dimensions. Because nMDS is iterative, is important to note that each time you produce an nMDS plot from scratch it may look slightly different, even when starting with the same data (although this is now less of a problem with increased computational power and the ability to set many initial random starts). It also means it can take a little bit of time to complete, depending on your device’s capabilities. View a summary of the results of the nMDS, which will display how the function was called (Call), the data set and any transformations used (Data), the distance measure (Distance), the number of dimensions (Dimensions), the final stress value (Stress), whether any convergent solution was achieved, how many random initial configurations were tried, plus whether scores have been scaled, centered, or rotated (Scaling). nmds_penguins_results # prints summary of results ## ## Call: ## metaMDS(comm = nmds_penguins, distance = &quot;euclidean&quot;, k = 2, try = 100, trymax = 100, autotransform = FALSE, noshare = FALSE, wascores = TRUE) ## ## global Multidimensional Scaling using monoMDS ## ## Data: nmds_penguins ## Distance: euclidean ## ## Dimensions: 2 ## Stress: 0.08608257 ## Stress type 1, weak ties ## Best solution was repeated 5 times in 100 tries ## The best solution was from try 0 (metric scaling or null solution) ## Scaling: centring, PC rotation ## Species: expanded scores based on &#39;nmds_penguins&#39; We can gain more details of the analysis. First, let’s call up a list of the names of the items in the nmds_penguins_results object. names (nmds_penguins_results) ## [1] &quot;nobj&quot; &quot;nfix&quot; &quot;ndim&quot; &quot;ndis&quot; &quot;ngrp&quot; ## [6] &quot;diss&quot; &quot;iidx&quot; &quot;jidx&quot; &quot;xinit&quot; &quot;istart&quot; ## [11] &quot;isform&quot; &quot;ities&quot; &quot;iregn&quot; &quot;iscal&quot; &quot;maxits&quot; ## [16] &quot;sratmx&quot; &quot;strmin&quot; &quot;sfgrmn&quot; &quot;dist&quot; &quot;dhat&quot; ## [21] &quot;points&quot; &quot;stress&quot; &quot;grstress&quot; &quot;iters&quot; &quot;icause&quot; ## [26] &quot;call&quot; &quot;model&quot; &quot;distmethod&quot; &quot;distcall&quot; &quot;data&quot; ## [31] &quot;distance&quot; &quot;converged&quot; &quot;tries&quot; &quot;bestry&quot; &quot;engine&quot; ## [36] &quot;species&quot; Below are some items created from the analysis that you may find useful (many are in the summary output above). Input these into R to see what information you learn about your analysis. (Remember the $ symbol means you are producing a subset from the total data) nmds_penguins_results$ndim #number of nMDS axes or dimensions created nmds_penguins_results$converged #did the iteration find a convergent solution (i.e. likely to have found global solution)? nmds_penguins_results$stress #stress value of final solution nmds_penguins_results$distance #distance metric used nmds_penguins_results$tries #number of random initial configurations tried nmds_penguins_results$points #scores for each sample (here, individual penguins) nmds_penguins_results$species #scores for variables (called species in the MetaMDS() function) We can produce a very basic exploratory nMDS plot in baseR with both sample and variable scores represented. plot (nmds_penguins_results) Open black circles correspond to samples and red crosses indicate variables. Axes have numbers on them but they are not interpretable from a nMDS – the scales are arbitrary values as we used rank order to separate the samples from each other. Because nothing is labelled, this default plot above is not very helpful. For substantially better plots, construct them yourself using ggplot2 by subsetting sample scores and variable scores from the nMDS output. The scores are the coordinates of samples and variables in the k dimensional space you have specified, so you can use them like any variable in a biplot. In the code below, we are creating some new data set objects containing the sample scores nmds_penguins_results$points and variable scores nmds_penguins_results$species information, and adding species and sex information from our original data set in case we wish to colour code samples in our ggplot based on this extra information - clusters or patterns may be related to this extra information. ### creating a nMDS plot using ggplot2 ## Recommendation is to run each line of code below and then view either the ## sample_scores or variable_scores objects to see what it is doing at each step # First create a data frame of the scores from the samples sample_scores &lt;- as.data.frame(nmds_penguins_results$points) # Now add the extra species and sex information columns # the line of code below says combine the current sample_scores object with # the first column of the penguins_nafree data set and store it in an object # called sample_scores sample_scores &lt;- cbind(sample_scores, penguins_nafree[, 1]) # this next line of code says add a column header to the third column (the new # column) of the sample_scores data frame; call it species # NOTE: check the sample_scores object you have created first. If it already # has species as column header, you do not need to run the next line of code colnames(sample_scores)[3] &lt;- &quot;species&quot; # based on the first two, can you work out what these two lines of code say? sample_scores &lt;- cbind(sample_scores, penguins_nafree[, 7]) colnames(sample_scores)[4] &lt;- &quot;sex&quot; # Next, we can create a data frame of the scores for variable data (species) variable_scores &lt;- as.data.frame(nmds_penguins_results$species) # Add a column equivalent to the row name to create variable labels on our plot variable_scores$variable &lt;- rownames(variable_scores) We will use the viridis package containing a colour-blind friendly colour map for creating our ggplot (you will need to install it first if you haven’t already – do you remember how to do that? HINT: install.packages(“”)): library(viridis) Now we have the elements we need to build the plot!! ggplot() + # the data points for each sample using the scores created in metaMDS function geom_point(data = sample_scores, aes(x = MDS1, y = MDS2, color = species), size = 3) + # create more appropriate x and y axis labels xlab(&quot;nMDS1&quot;) + ylab(&quot;nMDS2&quot;) + # what colours would we like? scale_color_manual(values = inferno(15)[c(3, 8, 11)], name = &quot;Species&quot;) + # add stress label to the plot, indicating the label position annotate(geom = &quot;label&quot;, x = -1.75, y = 2.75, size = 6, label = paste(&quot;Stress: &quot;, round(nmds_penguins_results$stress, digits = 3))) + theme_minimal() + # choosing a theme theme(legend.position = &quot;right&quot;, text = element_text(size = 10)) # position and size of legend We have not included variable labels from the variable_scores object we created on this plot (but we will in the next example!). Variable labels can help show which variables are similar/different between samples (i.e., which variables are influencing how the samples are separated in this 2-D space). We have created the variable score objects in the code above (variable_scores &lt;- ...), so please plot this information on to your ggplot if you’d like to look at it (use the example ggplot code in the macroinvertebrate example to see how to add the text labels to the plot). Once plotted, use the pairs plot colour coded by species to help you interpret the variable labels on the nMDS. You could ask what body composition variables appear important in distinguishing penguins by species (or sex)? How could you change this plot to be coloured by sex instead of by species? Give it a try. 8.7 Diagnostic plots for nMDS While there are limited assumptions with nMDS, it is still a good idea to check and see if the data are a good fit for the model. 8.7.1 Scree plot/Stress plot Stress is a goodness-of-fit statistic that nMDS tries to minimise. Stress takes a value between 0 and 1, with values near 0 indicating a better fit. It is calculated for each iteration of the algorithm and will move towards the lowest value to get the best solution. Stress increases with an increase in the number of samples and/or the number of variables. The stress value will always decrease with increased dimensionality because it is easier to project data into higher dimensions similar to what the original data came from. On the other hand, low-dimensional projections are often easier to interpret and are so preferable. It’s a trade-off. The stress plot (or sometimes also called a scree plot) is a diagnostic plot to explore both dimensionality and interpretative value. It plots stress versus number of dimensions to assist in identifying how many k dimensions are suitable to create an interpretable nMDS plot. # Stress plot/scree plot for nMDS library(goeveg) # needed for dimcheckMDS to create scree plot. Install and open dimcheckMDS(penguins_nafree[ , c(3:6)], # We are specifying columns 3-6 again # using subsetting brackets [] as this is the data we used for the nMDS distance = &quot;euclidean&quot;, # distance measure used for metaMDS analysis k = 3 # maximum number of dimensions, default is 6 but we only # have 4 original variables here so more than 3 does not make sense! ) Clarke (1993) suggests the following guidelines for acceptable stress values: The scree plot produced in R displays the border of the 0.20 stress value limit with a horizontal red dashed line). Solutions with higher stress values should be interpreted with caution and those with stress above 0.30 are highly suspect, showing basically random placement of data points. Remember, stress will always decrease with an increased number of dimensions, as we see in this scree plot above. The scree plot from our penguins data set shows stress values lower than the 0.20 stress limit for any of the 1-, 2-, or 3-D solutions. However, the stress value for the 1-D solution is approx. 0.18. This means our data is usable, but getting close to not being acceptable. Stress in 3-D is 0, likely due to only having 4 original variables that are quite highly correlated with each other, but is a bit more difficult to visualise and hence interpret than 2-D. Also the 2-D nMDS solution has a low stress value of around 0.08 so provides us with an interpretable solution that further reduces the number of dimensions from 4 to 2. So I’d say, let’s go with k = 2. We can determine the actual stress value by looking at the results outputs (nmds_penguins_results$stress 0.086), and we also added it to our ggplot. 8.7.2 Shepard Diagram A Shepard diagram is a scatterplot of distances between points in an nMDS final configuration (observed dissimilarity) against the original dissimilarities (ordination distance). For a nMDS, the line of best fit produced is monotonic, in that it can only step up or remain constant. The points in a good Shepard diagram should be a clean curve or straight line. Large scatter around the line suggests that original dissimilarities are not well preserved in the reduced number of dimensions. If your plot resembles a step-wise or inverse L-shaped function, it would be wise to not interpret this data and instead try to solve for a degenerate solution (not covered in this course). However, please note if you have a small data set (not many observations) your data may look step-wise in a Shepard diagram. Note, you will need to produce a new Shepard diagram when altering the number of dimensions being plotted. #confusingly, the Shepard diagram is called stressplot in R! stressplot(nmds_penguins_results) This Shepard diagram shows a gradual increase in the monotonic (red) line with an R2 value of 0.993 meaning this nMDS preserves the ranks of the original distance matrix really well (which is what we expected remember? Not many original variables and high pair-wise correlations). There is a little bit of scatter around the line but the majority of the observations are consistent with the overall trendline. Nothing to worry about here! 8.8 How to interpret an nMDS plot and what to report The interpretation of a (successful) nMDS is straightforward: the closer points are to each other the more similar is their community composition (or body composition for our penguin data, or whatever the variables represent). Look for clusters of samples or regular patterns among the samples. When writing up results from a nMDS, always include at least: A description of any data culling (e.g., we removed rows with NA values in our penguin data set) or data transformations that were used. State these in the order that they were performed. (e.g., after removing NA values from the penguin data set, we standardised/scaled then added a constant to the variables to make them positive) The distance measure, number of random restarts, and the specified number of dimensions/axes. The value of stress. An interpretation of the important/interesting trends and patterns in the data, made evident in the nMDS plot. And also include what is relevant from below: A table or plot of variable scores that shows how each variable contributes to each axis of the nMDS. you can add this information to your plot - use the code in the next example to guide you on how to do this. to create a table using the penguin data variable scores, try print.data.frame(variable_scores). if there are too many variables, this information may impede your ordination or not add anything useful to understanding your data. So use this with your discrepancy. Any other plots of sample scores that may be useful to aid your interpretation of the data, such as color-coding samples by a different factor variable (e.g., we colour coded by species, but hopefully you tried by sex too?). 8.9 nMDS in R using metaMDS() with community structure data Let’s now use a community structure data set - what this vegan package was designed for and so much simpler R code! We need to load the data, which consists of columns of variables/species and rows for samples. Each row-column combination is the abundance of that species in that sample. An FAQ is “what is community structure data?” It is in essence an ecological concept that uses species/taxa occurrence (what species/taxa is found where?) and abundance (how many individuals of each species/taxa are where?) information to provide species/taxa composition information. Or in other words, it tells us about the assemblages of the community found in that sample/location/site, both in terms of the types of species/taxa and also their abundances. Data is usually presented with each species/taxa in a separate column and each sample/location/site in a separate row. For this example, we are using some ecological data from the U.S. National Ecological Observatory Network’s aquatic macroinvertebrate collection (https://data.neonscience.org/data-products/DP1.20120.001). The data in the file provided to you in CANVAS are benthic macroinvertebrate species counts for rivers and lakes throughout the entire United States and were collected between July 2014 to October 2018. We are interested in investigating which macroinvertebrates are more associated with river systems and which are associated with lake systems. # loading packages below is only necessary if you&#39;re starting a new R session library(vegan) library(tidyverse) orders &lt;- read_csv(&quot;condensed_order.csv&quot;) # Note: for this code to work you need have downloaded this csv file from CANVAS # and saved it into your working directory # as you know, it’s always a good idea to check the data has # loaded correctly in R before proceeding further glimpse(orders) dim(orders) # how many variables and how many samples are there in this data set? # we can see this data set contains eight different animal orders, # location coordinates, type of aquatic system, and elevation. # We will consider the eight animal orders and the aquaticSiteType columns only. As we are interested in comparing the animal orders between aquatic systems (rivers vs lakes), we can create a pairs plot to look at the relationships between these animal order numeric variables. We can use the piping operator %&gt;% to do this without creating a new data frame. library(GGally) # only necessary if you&#39;re starting a new R session # pairs plot orders %&gt;% ggpairs(columns = 4:11) # specifies to only use variables from columns 4-11 in the orders data set # or we can add on the river and lake system information also! # to do this, we first need to convert aquaticSiteType into a factor variable orders$aquaticSiteType &lt;- as.factor(orders$aquaticSiteType) orders %&gt;% ggpairs(columns = 4:11, aes(colour = aquaticSiteType), # code below just makes correlation coefs font smaller so we can read it all! upper = list(continuous = wrap(&quot;cor&quot;, size = 2))) What can we learn from this plot? With eight original variables, we need 8-D to visualise this data accurately. We can see that the abundance of some animal orders are very low and some are very large (look at axes units) and some are only found in a few samples (look at the data set to get a better eye on this using glimpse(orders) or View(orders)). Looking at the correlation coefficients, we can see that there are not many strong correlations between variables. Using nMDS will retain the rank order of similarity between samples. However, as there are not strong correlations between variables, nMDS may not be able to do a good job at reducing dimensionality of the data. It might be a good idea to check our stress plot first to see how many dimensions we want to run our nMDS in. library(goeveg) #only necessary if you&#39;re starting a new R session dimcheckMDS(orders[ , 4:11], #data being used. Here we have specified columns # 4-11 again as this relates to the animal order data we used for the nMDS distance = &quot;bray&quot;, #dissimilarity index used for metaMDS analysis k = 6, #maximum number of dimensions trymax = 20, #maximum number of random configuration starts for #iterative search of the best solution ) This scree plot from the macroinvertebrate data set shows a stress value of around 0.18 when creating a 2-D nMDS. This means our data is usable, but getting close to not being acceptable. If we look back to the pairs plot, it showed no strong correlations between variables, we also have quite a large number of samples (dim(orders) to remind yourself of how many) which means we expected to have quite a high stress value in low dimensional space. However, because 2-D provides a usable stress value and it is the easiest for us to interpret, we will set k = 2. Right-o! So next we will run the nMDS, using the metaMDS() function with default settings for this community structure data set. But we never just trust defaults so let’s make sure we’re happy with what it is going to do for us! For example, the default distance measure is Bray-Curtis which is ideal for this type of species abundance data (if you want to specify this measure, we use the argument distance = “bray”). The default setting also uses common ecological data transformations which includes the Wisconsin double standardization and if values look very large, the function also performs a square-root transformation. These standardize samples by sample size (preventing large samples from dominating the analysis) and giving species equal weight (preventing abundant species from dominating the analysis). If you wanted to use already standardised and/or transformed data, or you want to do this step yourself using other transformations, you can set autotransform = FALSE. We will leave it at the default though because we have not pre-transformed this data and so do not need to specify anything. Remember, if you run into a problem where it can’t find a convergent solution (it can’t find the same solution at least two times), you can increase the number of random restarts to be more likely of finding the global solution for our specified k (e.g., increasing try = and trymax =). # run the analysis using chosen distance metric # because our chosen distance metric is BC, we do not need to specify distance # as BC is the default nmds_minv_results &lt;- metaMDS(comm = orders[ , 4:11], # Define the community data k = 2, # our stress plot showed this was ok try = 100, trymax = 100) # Number of random starts # no need to specify autotransform = , noshare = , or wascores = at all in our # code because for community data such as this, we can use the default settings. # We can get the same outputs as we did for the penguin data set # Remember what this all tells us? Look back over previous notes if required nmds_minv_results # print results ## ## Call: ## metaMDS(comm = orders[, 4:11], k = 2, try = 100, trymax = 100) ## ## global Multidimensional Scaling using monoMDS ## ## Data: wisconsin(sqrt(orders[, 4:11])) ## Distance: bray ## ## Dimensions: 2 ## Stress: 0.1756999 ## Stress type 1, weak ties ## Best solution was repeated 3 times in 100 tries ## The best solution was from try 88 (random start) ## Scaling: centring, PC rotation, halfchange scaling ## Species: expanded scores based on &#39;wisconsin(sqrt(orders[, 4:11]))&#39; #run this code to find this information: nmds_minv_results$ndim # how many nMDS axes were created? nmds_minv_results$converged # did it find a convergent solution? nmds_minv_results$points # scores for each location (as samples) nmds_minv_results$species # scores for each animal order (as variables) Here’s a basic exploratory plot for our macroinvertebrate community structure data… plot(nmds_minv_results) …but ggplots are much nicer and more informative! ### creating an nMDS plot using ggplot2 ## Recommendation is to run each line of code below and then view either the ## data_scores or species_scores objects to see what it is doing at each step #First create a data frame of the scores from the samples data_scores &lt;- as.data.frame(nmds_minv_results$points) # Now add the extra aquaticSiteType column # the line of code below says combine the current data_scores object with # the 14th column of the orders data set and store it in an object # called data_scores data_scores &lt;- cbind(data_scores, orders[, 14]) # this next line of code says add a column header to the third column (the new # column) of the data_scores data frame; call it aquaticSiteType # NOTE: check the sample_scores object you have created first. If it already # has aquaticSiteType as column header, you do not need to run the next line of code colnames(data_scores)[3] &lt;- &quot;aquaticSiteType&quot; # Next, we can create a data frame of the scores for species data species_scores &lt;- as.data.frame(nmds_minv_results$species) # Add a column equivalent to the row name to create species labels for us to plot species_scores$species &lt;- rownames(species_scores) # Now we can build the plot! ggplot() + # the text labels for each species group geom_text(data = species_scores, aes(x = MDS1, y = MDS2, label = species), alpha = 0.5, size = 6) + # the data points for each sample, using the scores created in metaMDS function geom_point(data = data_scores, aes(x = MDS1, y = MDS2, color = aquaticSiteType), size = 3) + # create more appropriate x and y axis labels xlab(&quot;nMDS1&quot;) + ylab(&quot;nMDS2&quot;) + # what colours would we like? (we only need two colours here) scale_color_manual(values = inferno(15)[c(3, 11)], name = &quot;Aquatic System Type&quot;) + # add stress label to the plot, indicating the label position and 3sf annotate(geom = &quot;label&quot;, x = -1.5, y = 1.5, size = 6, label = paste(&quot;Stress: &quot;, round(nmds_minv_results$stress, digits = 3))) + theme_minimal() + # choosing a theme theme(legend.position = &quot;right&quot;, text = element_text(size = 10)) # position and size of legend With community structure data, the variable labels can give a bit of information as to what species are more commonly found in which sample, which will be helpful to answer our research question we outlined before we started: which macroinvertebrates are more associated with river systems and which are associated with lake systems? What can you conclude from the plot we have created? What about the Shepard diagram (our other diagnostic plot)? #Shepard diagram stressplot(nmds_minv_results) What sorts of conclusions/interpretations can you make from these macroinvertebrate community structure nMDS outputs? Hint: use the information outlined in “How to interpret an nMDS plot and what to report” section of this course guide. 8.10 Other resources: optional but recommended Little book for Multivariate Analysis A 12 minute youtube video explaining nMDS 8.10.1 Creating a 3-D nMDS in R using vegan3d package (for future reference) Although not required for this course, you may come across data sets in the near future where stress levels are too high for a 2-D nMDS solution. So I thought I’d include some info for coding 3-D nMDS plots, it may come in handy one day! :-) You need to use an extension of the vegan package - it is called vegan3d. # remember to install the package if you haven&#39;t done so before library(vegan3d) We then need to run the metaMDS() function specifying k = 3 to get a 3-D solution. Let’s use the macroinvertebrate community structure data from the example above. The stress was much lower at 0.11 for the 3-D solution anyway - check out the stress plot. nmds_minv_results_3d &lt;- metaMDS(comm = orders[ , 4:11], k = 3, try = 100) After this, we need to subset the sample and variable scores from the output, which we did for the 2-D ggplot we created too. # sample scores data data_scores_3d &lt;-scores(nmds_minv_results_3d, display = &quot;sites&quot;) # variable scores data species_scores_3d &lt;-scores(nmds_minv_results_3d, display = &quot;species&quot;) from there it’s actually pretty straight forward code to get either a static 3-D plot using ordiplot3d… #static 3-D variable scores plot out &lt;- ordiplot3d(species_scores_3d, col = &quot;red&quot;, ax.col= &quot;black&quot;, pch = 18) text(out$xyz.convert(species_scores_3d), rownames(species_scores_3d), pos=1) #static 3-D sample scores plot with sample labels out2 &lt;- ordiplot3d(data_scores_3d, col = &quot;red&quot;, ax.col= &quot;black&quot;, pch = 18) text(out2$xyz.convert(data_scores_3d), rownames(data_scores_3d), pos=1) # too many samples to display labels and see anything useful in this static plot! # however, we can add variable labels instead - more useful #static 3-D sample scores plot with variable labels out3 &lt;- ordiplot3d(data_scores_3d, col = &quot;red&quot;, ax.col= &quot;black&quot;, pch = 18) text(out3$xyz.convert(species_scores_3d), rownames(species_scores_3d), pos=1) … or you can create a rotatable, zoomable dynamic 3-D plot using ordirgl which won’t display in this course guide but are useful and easy to rotate and manipulate on your computer screen using the code below. #rotatable 3-D sample scores plot with sample points ordirgl(data_scores_3d, col = &quot;red&quot;, type = &quot;p&quot;, ax.col= &quot;black&quot;, pch = 18) # to change points to sample labels use type = &quot;t&quot; #rotatable 3-D variable scores plot with variable labels ordirgl(species_scores_3d, col = &quot;red&quot;, type = &quot;t&quot;, ax.col= &quot;black&quot;, pch = 18) Note: these functions all work fine on my Dell laptop. However, there is a warning in the documentation that function ordirgl uses another package rgl which may not be functional in all platforms, and can crash R in some. Mac users must start X11 (and first install X11 and some other libraries) before being able to use rgl. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
